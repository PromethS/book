# 分布式系统设计思路

![image-20200605004541196](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605004543.png)

## 中心化设计

两个角色： 中心化的设计思想很简单，分布式集群中的节点机器按照角色分工，大体上分为两种角色： **“领导” 和 “干活的”**

角色职责： “领导”通常负责分发任务并监督“干活的”，发现谁太闲了，就想发设法地给其安排新任务，确保没有一个“干活的”能够偷懒，如果“领导”发现某个“干活的”因为劳累过度而病倒了，则是不会考虑先尝试“医治”他的，而是**一脚踢出去**，然后把他的任务分给其他人。其中微服务架构 **Kubernetes** 就恰好采用了这一设计思路。

中心化设计的问题：

中心化的设计存在的最大问题是“领导”的安危问题，**如果“领导”出了问题**，则群龙无首，整个集群就奔溃了。但我们难以同时安排两个“领导”以避免单点问题。

中心化设计还存在另外一个潜在的问题，**既“领导”的能力问题**：可以领导10个人高效工作并不意味着可以领导100个人高效工作，所以如果系统设计和实现得不好，问题就会卡在“领导”身上。

领导安危问题的解决办法： 大多数中心化系统都采用了**主备**两个“领导”的设计方案，可以是热备或者冷备，也可以是自动切换或者手动切换，而且越来越多的新系统都开始具备**自动选举**切换“领导”的能力，以提升系统的可用性。

## 去中心化设计
众生地位平等： 在去中心化的设计里，通常没有“领导”和“干活的”这两种角色的区分，大家的角色都是一样的，地位是平等的，全球互联网就是一个典型的去中心化的分布式系统，联网的任意节点设备宕机，都只会影响很小范围的功能。

“去中心化”不是不要中心，而是由节点来**自由选择中心**。 （集群的成员会自发的举行“会议”选举新的“领导”主持工作。）

> 最典型的案例就是**ZooKeeper**、Go语言实现的**Etcd**、redis cluster集群版

去中心化设计的问题： 去中心化设计里最难解决的一个问题是 **“脑裂”问题** ，这种情况的发生概率很低，但影响很大。脑裂问题，这种情况的发生概率很低，但影响很大。脑裂指一个集群由于网络的故障，被分为至少两个彼此无法通信的单独集群，此时如果两个集群都各自工作，则可能会产生严重的数据冲突和错误。一般的设计思路是，当集群判断发生了脑裂问题时，**规模较小的集群就“自杀”或者拒绝服务**。

## 分布式与集群的区别

- **分布式：** **一个业务分拆多个子业务**，部署在不同的服务器上
- **集群：** **同一个业务，部署在多个服务器上**。比如之前做电商网站搭的redis集群以及solr集群都是属于将redis服务器提供的缓存服务以及solr服务器提供的搜索服务部署在多个服务器上以提高系统性能、并发量解决海量存储问题。

# CAP

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605003158.jpg)

在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：

- **一致性**（Consistence） :所有节点访问同一份最新的数据副本
- **可用性**（Availability）:每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据
- **分区容错性**（Partition tolerance） : 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。

**CAP仅适用于原子读写的NOSQL场景中**，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。

**注意：**不是所谓的3选2（不要被网上大多数文章误导了）:

大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。

当发生网络分区的时候，如果我们要继续服务，**那么强一致性和可用性只能2选1**。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。

# BASE

BASE理论由**eBay架构师**Dan Pritchett提出，在2008年上被分表为论文，并且eBay给出了他们在实践中总结的基于BASE理论的一套新的分布式事务解决方案。

BASE 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。

**核心思想：**

即使**无法做到强一致性**，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到**最终一致性**。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。

针对数据库领域，BASE思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：

- 按功能划分数据库
- 分片（如开源的Mycat、Amoeba等）。

由于拆分后会涉及分布式事务问题，所以eBay在该BASE论文中提到了如何用最终一致性的思路来实现高性能的分布式事务。

**基本可用：**

基本可用是指分布式系统在出现不可预知故障的时候，**允许损失部分可用性**。但是，这绝不等价于系统不可用。

比如：

- 响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒
- 系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

**软状态：**

软状态指允许系统中的数据**存在中间状态**，并认为该中间状态的存在**不会影响系统的整体可用性**，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时

**最终一致性：**

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

# 分布式事务

分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（**全部提交或全部回滚**）

## XA规范

XA 就是 X/Open DTP 定义的交易**中间件与数据库**之间的接口规范（即接口函数），交易中间件用它来通知数据库**事务的开始、结束以及提交、回滚等**。 XA 接口函数由数据库厂商提供。 

**二阶提交协议**和**三阶提交协议**就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现**XA分布式事务**的关键(确切地说：两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做)

## 2PC

**二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。**

所谓的两个阶段是指：第一阶段：**准备阶段(投票阶段)**和第二阶段：**提交阶段（执行阶段）**。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605005118.png)

**准备阶段：**

​	事务协调者(事务管理器)给每个参与者(资源管理器)发送**Prepare消息**，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，**写本地的redo和undo日志**，但不提交，到达一种“万事俱备，只欠东风”的状态。

**提交阶段：**

​	如果协调者收到了参与者的失败消息或者超时，直接给每个参与者**发送回滚(Rollback)消息**；否则，**发送提交(Commit)消息**；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)

**总的来说：**

​	**XA协议比较简单**，而且一旦商业数据库实现了XA协议，使用分布式事务的成本也比较低。

​	但是，XA也有**致命的缺点**，那就是**性能不理想**，特别是在交易下单链路，往往并发量很高，XA**无法满足高并发**场景。

​	XA目前在商业数据库支持的比较理想，**在mysql数据库中支持的不太理想**，mysql的XA实现，没有记录prepare阶段日志，主备切换回导致主库与备库数据不一致。许多nosql也没有支持XA，这让XA的应用场景变得非常狭隘。

## 3PC

三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。

与两阶段提交不同的是，三阶段提交有两个改动点。

- 1、引入**超时机制**。同时在协调者和参与者中都引入超时机制。
- 2、3PC把2PC的准备阶段再次一分为二，**插入了一个准备阶段**。保证了在最后提交阶段之前各参与节点的状态是一致的。

**CanCommit阶段：**

​	3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

> **1.事务询问** 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
>
> **2.响应反馈** 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No

**PreCommit阶段：**

​	协调者根据参与者的反应情况来决定是否可以发起事务的PreCommit操作。

​	**假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。**

> **1.发送预提交请求** 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
>
> **2.事务预提交** 参与者接收到PreCommit请求后，会执行事务操作，并将**undo和redo**信息记录到事务日志中。
>
> **3.响应反馈** 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

​	**假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。**

> **1.发送中断请求** 协调者向所有参与者发送abort请求。
>
> **2.中断事务** 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。

**doCommit阶段：**

​	该阶段进行真正的事务提交，在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，**会在等待超时之后，会继续进行事务的提交。**

> **1.发送提交请求** 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
>
> **2.事务提交** 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
>
> **3.响应反馈** 事务提交完之后，向协调者发送Ack响应。
>
> **4.完成事务** 协调者接收到所有参与者的ack响应之后，完成事务。

**2PC与3PC的区别：**

​	相对于2PC，3PC主要**解决的是单点故障问题，并减少阻塞**，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制**也会导致数据一致性问题**，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

## TCC-补偿性事务

TCC 其实就是采用的补偿机制，其核心思想是：**针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作**。它分为三个阶段：

- **Try 阶段**主要是对业务系统做检测及资源预留
- **Confirm 阶段**主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。
- **Cancel 阶段**主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。

举个例子，假入 Bob 要向 Smith 转账，思路大概是：

我们有一个本地方法，里面依次调用

1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。

2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。

3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

**优点：** 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些

**缺点：** 缺点还是比较明显的，**在2,3步中都有可能失败**。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多**写很多补偿的代码**，在一些场景中，一些业务流程可能用TCC不太好定义及处理。

## 本地消息表（异步确保）

**本地消息表这种实现方式应该是业界使用最多的**，其核心思想是将分布式事务**拆分成本地事务**进行处理，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节：

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605010708.png)

基本思路就是：

消息生产方，需要额外建一个消息表，并记录消息发送状态。**消息表和业务数据要在一个事务里提交**，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行**重试发送**。

消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会**重试执行**。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，**通知生产方进行回滚**等操作。

生产方和消费方**定时扫描本地消息表**，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。

这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。

**优点：** 一种非常经典的实现，避免了分布式事务，实现了最终一致性。

**缺点：** 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

## MQ 事务消息

有一些第三方的MQ是支持事务消息的，比如**RocketMQ**，他们支持事务消息的方式也是**类似于二阶段提交**，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。

以阿里的 RocketMQ 中间件为例，其思路大致为：

第一阶段Prepared消息，会拿到消息的地址。
第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。

也就是说在业务方法内要想消息队列提交两次请求，**一次发送消息和一次确认消息**。如果确认消息发送失败了RocketMQ会定期扫描消息集群中的事务消息，这时候发现了Prepared消息，它会向消息发送者确认，所以生产方需要实现一个check接口，RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605011021.png)

**优点：** 实现了最终一致性，不需要依赖本地数据库事务。

**缺点：** 实现难度大，主流MQ不支持，RocketMQ事务消息部分代码也未开源。

## seata

Seata(Simple Extensible Autonomous Transaction Architecture) 是 阿里巴巴开源的分布式事务中间件，以高效并且对业务 0 侵入的方式，解决微服务场景下面临的分布式事务问题。

由**一批分布式事务组成的全局事务**，通常分支事务只是本地事务。

**分为两种实现方案：**

AT模式：基于XA协议，依赖数据库支持XA协议，对业务侵入性最小，主要针对于DB事务

> AT模式，是采用**本地生成回滚日志**，先把本地事务提交，避免本地事务的锁等待。接收到全局事务回滚通知后，基于回滚日志回滚数据

TCC模式：基于TCC补偿性事务，主要关注业务拆分，解决微服务服务间调用的一致性问题

### AT模式(业务侵入小)

Seata AT模式是基于XA事务演进而来的一个分布式事务中间件，XA是一个基于数据库实现的分布式事务协议，本质上和两阶段提交一样，需要数据库支持，**Mysql5.6以上版本支持XA协议**，其他数据库如Oracle，DB2也实现了XA接口。

两阶段提交协议的演变：

- 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。
- 二阶段：
  - 提交异步化，非常快速地完成。
  - 回滚通过一阶段的回滚日志进行反向补偿。

由事务协调器（TC）、事务管理器（TM）、资源管理器（RM）组成。**使用zookeeper统一调度**。

1）事务协调器（TC）：**维护全局事务的运行状态**；

2）事务管理器（TM）：**控制事务**，负责开启全局事务、最终发起全局提交或全局回滚；

3）资源管理器（RM）：本地事务的管理者，负责本地事务注册、状态汇报、提交或回滚	

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605015322.png)

**第一阶段**

Seata 的 JDBC 数据源**代理通过对业务 SQL 的解析**，把业务数据在**更新前后的数据镜像组织成回滚日志**，利用本地事务的 ACID 特性，将业务数据的更新和回滚日志的写入在同一个 本地事务 中提交。

这样，可以保证：任何提交的业务数据的更新一定有相应的回滚日志存在

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605015535.png)

基于这样的机制，分支的本地事务便可以在全局事务的第一阶段提交，并**马上释放本地事务锁定的资源**

这也是Seata和XA事务的不同之处，两阶段提交往往对资源的锁定需要持续到第二阶段实际的提交或者回滚操作，而有了回滚日志之后，可以在第一阶段释放对资源的锁定，降低了锁范围，提高效率，即使第二阶段发生异常需要回滚，只需找对undolog中对应数据并反解析成sql来达到回滚目的

同时Seata通过代理数据源将业务sql的执行**解析成undolog**来与业务数据的更新同时入库，达到了对业务无侵入的效果

**第二阶段**

如果决议是全局提交，此时分支事务此时已经完成提交，不需要同步协调处理（只需要异步清理回滚日志），Phase2 可以非常快速地完成

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605015620.png)
如果决议是全局回滚，RM 收到协调器发来的回滚请求，通过 XID 和 Branch ID 找到相应的回滚日志记录，通过回滚记录生成反向的更新 SQL 并执行，以完成分支的回滚

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605015637.png)

#### 写隔离

- 一阶段本地事务提交前，需要确保先拿到 **全局锁** 。
- 拿不到 **全局锁** ，不能提交本地事务。
- 拿 **全局锁** 的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。

以一个示例来说明：

两个全局事务 tx1 和 tx2，分别对 a 表的 m 字段进行更新操作，m 的初始值 1000。

tx1 先开始，开启本地事务，拿到本地锁，更新操作 m = 1000 - 100 = 900。本地事务提交前，先拿到该记录的 **全局锁** ，本地提交释放本地锁。 

tx2 后开始，开启本地事务，拿到本地锁，更新操作 m = 900 - 100 = 800。本地事务提交前，尝试拿该记录的 **全局锁** ，tx1 本地提交前，该记录的全局锁被 tx1 持有，tx2 需要重试等待 **全局锁** 。

tx1 二阶段全局提交，释放 **全局锁** 。tx2 拿到 **全局锁** 提交本地事务。

![Write-Isolation: Commit](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605021022.png)



如果 tx1 的二阶段全局回滚，则 tx1 需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支的回滚。

此时，如果 tx2 仍在等待该数据的 **全局锁**，同时持有本地锁，则 tx1 的分支回滚会失败。分支的回滚会一直重试，直到 tx2 的 **全局锁** 等锁超时，放弃 **全局锁** 并回滚本地事务释放本地锁，tx1 的分支回滚最终成功。

因为整个过程 **全局锁** 在 tx1 结束前一直是被 tx1 持有的，所以不会发生 **脏写** 的问题。

![Write-Isolation: Rollback](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605021125.png)



#### 读隔离

在数据库本地事务隔离级别 **读已提交（Read Committed）** 或以上的基础上，Seata（AT 模式）的默认全局隔离级别是 **读未提交（Read Uncommitted）** 。

如果应用在特定场景下，必需要求全局的 **读已提交** ，目前 Seata 的方式是通过 SELECT FOR UPDATE 语句的代理。

![Read Isolation: SELECT FOR UPDATE](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605021355.png)

SELECT FOR UPDATE 语句的执行会申请 **全局锁** ，如果 **全局锁** 被其他事务持有，则释放本地锁（回滚 SELECT FOR UPDATE 语句的本地执行）并重试。这个过程中，查询是被 block 住的，直到 **全局锁** 拿到，即读取的相关数据是 **已提交** 的，才返回。

出于总体性能上的考虑，Seata 目前的方案并没有对所有 SELECT 语句都进行代理，仅针对 FOR UPDATE 的 SELECT 语句。

#### 工作机制

以一个示例来说明整个 AT 分支的工作过程。

业务表：`product`

| Field | Type         | Key  |
| ----- | ------------ | ---- |
| id    | bigint(20)   | PRI  |
| name  | varchar(100) |      |
| since | varchar(100) |      |

AT 分支事务的业务逻辑：

```sql
update product set name = 'GTS' where name = 'TXC';
```

##### 一阶段

过程：

1. 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = 'TXC'）等相关的信息。
2. 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。

```sql
select id, name, since from product where name = 'TXC';
```

得到前镜像：

| id   | name | since |
| ---- | ---- | ----- |
| 1    | TXC  | 2014  |

1. 执行业务 SQL：更新这条记录的 name 为 'GTS'。
2. 查询后镜像：根据前镜像的结果，通过 **主键** 定位数据。

```sql
select id, name, since from product where id = 1`;
```

得到后镜像：

| id   | name | since |
| ---- | ---- | ----- |
| 1    | GTS  | 2014  |

1. 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 `UNDO_LOG` 表中。

```json
{
	"branchId": 641789253,
	"undoItems": [{
		"afterImage": {
			"rows": [{
				"fields": [{
					"name": "id",
					"type": 4,
					"value": 1
				}, {
					"name": "name",
					"type": 12,
					"value": "GTS"
				}, {
					"name": "since",
					"type": 12,
					"value": "2014"
				}]
			}],
			"tableName": "product"
		},
		"beforeImage": {
			"rows": [{
				"fields": [{
					"name": "id",
					"type": 4,
					"value": 1
				}, {
					"name": "name",
					"type": 12,
					"value": "TXC"
				}, {
					"name": "since",
					"type": 12,
					"value": "2014"
				}]
			}],
			"tableName": "product"
		},
		"sqlType": "UPDATE"
	}],
	"xid": "xid:xxx"
}
```

1. 提交前，向 TC 注册分支：申请 `product` 表中，主键值等于 1 的记录的 **全局锁** 。
2. 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。
3. 将本地事务提交的结果上报给 TC。

##### 二阶段-回滚

1. 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。
2. 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。
3. 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，**如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理**，详细的说明在另外的文档中介绍。
4. 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句：

```sql
update product set name = 'TXC' where id = 1;
```

1. 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。

##### 二阶段-提交

1. 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。
2. 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。

#### 优缺点

##### 亮点

相比与其它分布式事务框架，Seata架构的亮点主要有几个:

1. 应用层基于SQL解析**实现了自动补偿**，从而最大程度的降低业务侵入性；
2. 将分布式事务中TC（事务协调者）独立部署，负责事务的注册、回滚；
3. **通过全局锁实现了写隔离与读隔离**。

##### 性能损耗

我们看看Seata增加了哪些开销（纯内存运算类的忽略不计）：

一条Update的SQL，则需要全局事务xid获取（与TC通讯）、before image（解析SQL，查询一次数据库）、after image（查询一次数据库）、insert undo log（写一次数据库）、before commit（与TC通讯，判断锁冲突），这些操作都需要一次远程通讯RPC，而且是同步的。另外undo log写入时blob字段的插入性能也是不高的。**每条写SQL都会增加这么多开销,粗略估计会增加5倍响应时间**（二阶段虽然是异步的，但其实也会占用系统资源，网络、线程、数据库）。

##### 性价比

为了进行自动补偿，需要对所有交易生成前后镜像并持久化，可是在实际业务场景下，这个是成功率有多高，或者说分布式事务失败需要回滚的有多少比率？这个比例在不同场景下是不一样的，考虑到执行事务编排前，很多都会校验业务的正确性，所以发生回滚的概率其实相对较低。按照二八原则预估，**即为了20%的交易回滚，需要将80%的成功交易的响应时间增加5倍，这样的代价相比于让应用开发一个补偿交易是否是值得？值得我们深思。**

> 业界还有种思路，**通过数据库binlog恢复SQL执行前后镜像**，这样省去了同步undo log生成记录，减少了性能损耗，同时对业务零侵入，个人感觉是一种更好的方式。

##### 热点数据问题

Seata在每个分支事务中会携带对应的锁信息，在before commit阶段会依次**获取锁**(因为需要将所有SQL执行完才能拿到所有锁信息，所以放在commit前判断)。相比XA，Seata 虽然在一阶段成功后会释放数据库锁，但一阶段在commit前全局锁的判定也拉长了对数据锁的占有时间，这个开销比XA的prepare低多少需要根据实际业务场景进行测试。**全局锁的引入实现了隔离性，但带来的问题就是阻塞，降低并发性，尤其是热点数据**，这个问题会更加严重。

##### 死锁问题

Seata的引入全局锁会额外增加死锁的风险，但如果出现死锁，会**不断进行重试**，最后靠等待全局锁超时，这种方式并不优雅，也延长了对数据库锁的占有时间。

### TCC(高性能)

seata也针对TCC做了适配兼容，支持TCC事务方案，基本思路就是使用侵入业务上的补偿及事务管理器的协调来达到全局事务的一起提交及回滚。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200605015710.png)

根据两阶段行为模式的不同，我们将分支事务划分为 **Automatic (Branch) Transaction Mode** 和 **TCC (Branch) Transaction Mode**.

AT 模式基于 **支持本地 ACID 事务** 的 **关系型数据库**：

- 一阶段 prepare 行为：在本地事务中，一并提交业务数据更新和相应回滚日志记录。
- 二阶段 commit 行为：马上成功结束，**自动** 异步批量清理回滚日志。
- 二阶段 rollback 行为：通过回滚日志，**自动** 生成补偿操作，完成数据回滚。

相应的，TCC 模式，不依赖于底层数据资源的事务支持：

- 一阶段 prepare 行为：调用 **自定义** 的 prepare 逻辑。
- 二阶段 commit 行为：调用 **自定义** 的 commit 逻辑。
- 二阶段 rollback 行为：调用 **自定义** 的 rollback 逻辑。

所谓 TCC 模式，是指支持把 **自定义** 的分支事务纳入到全局事务的管理中。