# paxos

《[分布式系列文章——Paxos算法原理与推导（图文完整版）](https://mp.weixin.qq.com/s?__biz=MzI0NDI0MTgyOA==&mid=2652037784&idx=1&sn=d8c4f31a9cfb49ee91d05bb374e5cdd5&chksm=f2868653c5f10f45fc4a64d15a5f4163c3e66c00ed2ad334fa93edb46671f42db6752001f6c0#rd)》

## Paxos是什么

Paxos算法是基于**消息传递**且具有**高度容错特性**的**一致性算法**，是目前公认的解决**分布式一致性**问题**最有效**的算法之一。

Google Chubby的作者Mike Burrows说过这个世界上**只有一种**一致性算法，那就是Paxos，其它的算法都是**残次品**。

虽然Mike Burrows说得有点夸张，但是至少说明了Paxos算法的地位。然而，Paxos算法也因为晦涩难懂而臭名昭著。

在Paxos算法中，有三种角色：

- **Proposer**

  Proposer可以提出（propose）提案

- **Acceptor**

  Acceptor可以接受（accept）提案

- **Learners**

  不参与Paxos提案选定的过程，只在提案被选定时，知道提案结果的角色。

在具体的实现中，一个进程可能**同时充当多种角色**。比如一个进程可能**既是Proposer又是Acceptor又是Learner**。

还有一个很重要的概念叫**提案（Proposal）**。最终要达成一致的value就在提案里。

## 算法描述

经过上面的推导，我们总结下Paxos算法的流程。

Paxos算法分为**两个阶段**。具体如下：

- **阶段一：**

  (a) Proposer选择一个**提案编号N**，然后向**半数以上**的Acceptor发送编号为N的**Prepare请求**。

  (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N**大于**该Acceptor已经**响应过的**所有**Prepare请求**的编号，那么它就会将它已经**接受过的编号最大的提案（如果有的话）**作为响应反馈给Proposer，同时该Acceptor承诺**不再接受**任何**编号小于N的提案**。

- **阶段二：**

  (a) 如果Proposer收到**半数以上**Acceptor对其发出的编号为N的Prepare请求的**响应**，那么它就会发送一个针对**[N,V]提案**的**Accept请求**给**半数以上**的Acceptor。注意：V就是收到的**响应**中**编号最大的提案的value**，如果响应中**不包含任何提案**，那么V就由Proposer**自己决定**。

  (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor**没有**对编号**大于N**的**Prepare请求**做出过**响应**，它就**接受该提案**。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200607173346)

## Learner学习被选定的value

Learner学习（获取）被选定的value有如下三种方案：

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200607173354)

## 如何保证Paxos算法的活性

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200607173418)

通过选取**主Proposer**，就可以保证Paxos算法的活性。至此，我们得到一个**既能保证安全性，又能保证活性**的**分布式一致性算法**——**Paxos算法**。

# raft

## 概述

### 角色划分

**Raft是一个用于管理日志一致性的协议**。它将分布式一致性分解为多个子问题：**Leader选举（Leader election）、日志复制（Log replication）、安全性（Safety）、日志压缩（Log compaction）等**。同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。**Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选者**（Candidate）：

- Leader：**接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志**。
- Follower：**接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志**。
- Candidate：**Leader选举过程中的临时角色**。

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608012453.jpg)

Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。Raft算法将时间分为一个个的**任期（term）**，每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。

### term

**Raft 算法将时间划分成为任意不同长度的任期（term）**。任期用连续的数字进行表示。**每一个任期的开始都是一次选举（election），一个或多个候选人会试图成为领导人**。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。**Raft 算法保证在给定的一个任期最多只有一个领导人**。

### RPC

**Raft 算法中服务器节点之间通信使用远程过程调用（RPC）**，并且基本的一致性算法只需要两种类型的 RPC，为了在服务器之间传输快照增加了第三种 RPC。

【RPC有三种】：

- **RequestVote RPC**：**候选人在选举期间发起**。
- **AppendEntries RPC**：**领导人发起的一种心跳机制，复制日志也在该命令中完成**。
- **InstallSnapshot RPC**: 领导者使用该RPC来**发送快照给太落后的追随者**。

## Leader 选举

### 选举过程

**Raft 使用心跳（heartbeat）触发Leader选举**。当服务器启动时，初始化为Follower。**Leader**向所有**Followers**周期性发送心跳。**如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举**。

**每一个follower都有一个时钟，是一个随机的值，表示的是follower等待成为leader的时间，谁的时钟先跑完，则发起leader选举**。

**Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC**。结果有以下三种情况：

- **赢得了多数的选票，成功选举为Leader**；
- 收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；
- 没有服务器赢得多数的选票，Leader选举失败，等待选举时间**超时后发起下一次选举**。

### 选举的限制

**在Raft协议中，所有的日志条目都只会从Leader节点往Follower节点写入，且Leader节点上的日志只会增加，绝对不会删除或者覆盖**。

这意味着Leader节点必须包含所有已经提交的日志，即能被选举为Leader的节点一定需要包含所有的已经提交的日志。因为日志只会从Leader向Follower传输，所以如果被选举出的Leader缺少已经Commit的日志，那么这些已经提交的日志就会丢失，显然这是不符合要求的。

这就是Leader选举的限制：**能被选举成为Leader的节点，一定包含了所有已经提交的日志条目**。

![](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013144.png)

### 总结

**简而言之，leader选举的过程是：1、增加term号；2、给自己投票；3、重置选举超时计时器；4、发送请求投票的RPC给其它节点**。

## 日志复制（保证数据一致性）

### 复制过程

 **Leader选出后，就开始接收客户端的请求**。**Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC复制日志条目**。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。

- 客户端的每一个请求都包含被复制状态机执行的指令。
- **leader把这个指令作为一条新的日志条目添加到日志中，然后并行发起 RPC 给其他的服务器，让他们复制这条信息**。
- 假如这条日志被安全的复制，领导人就应用这条日志到自己的状态机中，并返回给客户端。
- **如果 follower 宕机或者运行缓慢或者丢包，leader会不断的重试，直到所有的 follower 最终都复制了所有的日志条目**。

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013137.png)

### 日志的组成

**日志由有序编号（log index）的日志条目组成**。**每个日志条目包含它被创建时的任期号（term）和用于状态机执行的命令**。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013207.png)

### 日志的一致性

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013242.jpg)

#### （1）日志复制的两条保证

- 如果不同日志中的两个条目有着**相同的索引和任期号，则它们所存储的命令是相同的**（原因：**leader 最多在一个任期里的一个日志索引位置创建一条日志条目，日志条目在日志的位置从来不会改变**）。
- 如果不同日志中的两个条目有着**相同的索引和任期号，则它们之前的所有条目都是完全一样的**（原因：**每次 RPC 发送附加日志时**，leader 会把这条日志条目的前面的**日志的下标和任期号一起发送给 follower**，如果 **follower 发现和自己的日志不匹配，那么就拒绝接受这条日志**，这个称之为**一致性检查**）。

#### （2）日志的不正常情况

一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：**旧的Leader可能没有完全复制完日志中的所有条目**。

下图阐述了一些Followers可能和新的Leader日志不同的情况。**一个Follower可能会丢失掉Leader上的一些条目，也有可能包含一些Leader没有的条目，也有可能两者都会发生**。丢失的或者多出来的条目可能会持续多个任期。

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013351.jpg)

#### （3）如何保证日志的正常复制

  Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。**Leader为了使Followers的日志同自己的一致，Leader需要找到Followers同它的日志一致的地方，然后覆盖Followers在该位置之后的条目**。

  具体的操作是：**Leader会从后往前试**，每次AppendEntries失败后尝试前一个日志条目，**直到成功找到每个Follower的日志一致位置点（基于上述的两条保证），然后向后逐条覆盖Followers在该位置之后的条目**。

### 总结

总结一下就是：**当 leader 和 follower 日志冲突的时候**，leader 将**校验 follower 最后一条日志是否和 leader 匹配**，如果不匹配，**将递减查询，直到匹配，匹配后，删除冲突的日志**。这样就实现了主从日志的一致性。

##  安全性

Raft增加了如下两条限制以保证安全性：

- 拥有**最新的已提交的log entry的Follower才有资格成为leader**。
- **Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志**，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。

## 日志压缩

在实际的系统中，**不能让日志无限增长**，否则**系统重启时需要花很长的时间进行回放**，从而影响可用性。**Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃（以前的数据已经落盘了）**。

**每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot**。

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013644.png)

【**Snapshot中包含以下内容**】：

- **日志元数据，最后一条已提交的 log entry的 log index和term**。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。
- **系统当前状态**。

当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC。

做snapshot既不要做的太频繁，否则**消耗磁盘带宽**， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。**推荐当日志达到某个固定的大小做一次snapshot**。

做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步。

## 成员变更（扩容/缩容）

我们先将成员变更请求当成普通的写请求，由领导者得到多数节点响应后，每个节点提交成员变更日志，将从旧成员配置（Cold）切换到新成员配置（Cnew）。但每个节点提交成员变更日志的时刻可能不同，这将造成各个服务器切换配置的时刻也不同，这就有可能选出两个领导者，破坏安全性。

考虑以下这种情况：集群配额从 3 台机器变成了 5 台，**可能存在这样的一个时间点，两个不同的领导者在同一个任期里都可以被选举成功（双主问题）**，**一个是通过旧的配置，一个通过新的配置**。

**简而言之，成员变更存在的问题是增加或者减少的成员太多了，导致旧成员组和新成员组没有交集，因此出现了双主**。

**Raft解决方法是每次成员变更只允许增加或删除一个成员（如果要变更多个成员，连续变更多次）**。

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608013857.png)

## 面试题

### 1、Raft分为哪几个部分？

  **主要是分为leader选举、日志复制、日志压缩、成员变更等**。

### 2、Raft中任何节点都可以发起选举吗？

Raft发起选举的情况有如下几种：

- 刚启动时，所有节点都是follower，这个时候发起选举，选出一个leader；
- 当leader挂掉后，**时钟最先跑完的follower发起重新选举操作**，选出一个新的leader。
- 成员变更的时候会发起选举操作。

### 3、Raft中选举中给候选人投票的前提？

  **Raft确保新当选的Leader包含所有已提交（集群中大多数成员中已提交）的日志条目**。这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的**last log entry的term_id和index**，follower在接收到RequestVoteRPC消息时，**如果发现自己的日志比RPC中的更新，就拒绝投票**。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。

### 4、Raft网络分区下的数据一致性怎么解决？

  发生了网络分区或者网络通信故障，**使得Leader不能访问大多数Follwer了，那么Leader只能正常更新它能访问的那些Follower，而大多数的Follower因为没有了Leader，他们重新选出一个Leader**，然后这个 Leader来接受客户端的请求，如果客户端要求其添加新的日志，这个新的Leader会通知大多数Follower。**如果这时网络故障修复 了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新（递减查询匹配日志）**。
![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608014054.png)

### 5、Raft数据一致性如何实现？

**主要是通过日志复制实现数据一致性，leader将请求指令作为一条新的日志条目添加到日志中，然后发起RPC 给所有的follower，进行日志复制，进而同步数据**。

### 6、Raft的日志有什么特点？

**日志由有序编号（log index）的日志条目组成，每个日志条目包含它被创建时的任期号（term）和用于状态机执行的命令**。

### 7、Raft和Paxos的区别和优缺点？

- Raft的leader有限制，**拥有最新日志的节点才能成为leader**，multi-paxos中对成为Leader的限制比较低，**任何节点都可以成为leader**。
- **Raft中Leader在每一个任期都有Term**号。

### 8、Raft prevote（预投票）机制？

![在这里插入图片描述](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608014202.png)
**Prevote（预投票）是一个类似于两阶段提交的协议**，**第一阶段先征求其他节点是否同意选举，如果同意选举则发起真正的选举操作，否则降为Follower角色**。这样就**避免了网络分区节点重新加入集群，触发不必要的选举操作**。

### 9、Raft里面怎么保证数据被commit，leader宕机了会怎样，之前的没提交的数据会怎样？

**leader会通过RPC向follower发出日志复制，等待所有的follower复制完成，这个过程是阻塞的**。

**老的leader里面没提交的数据会回滚，然后同步新leader的数据**。

### 10、Raft日志压缩是怎么实现的？增加或删除节点呢？？

在实际的系统中，**不能让日志无限增长**，否则**系统重启时需要花很长的时间进行回放**，从而影响可用性。**Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃（以前的数据已经落盘了）**。

**snapshot里面主要记录的是日志元数据，即最后一条已提交的 log entry的 log index和term**。

# ZAB

## zookeeper

ZooKeeper是一个**分布式协调服务**，可用于服务发现、分布式锁、分布式领导选举、配置管理等。

这一切的基础，都是ZooKeeper提供了一个类似于Linux文件系统的**树形结构**（可认为是轻量级的内存文件系统，但**只适合存少量信息**，完全不适合存储大量文件或者大文件），同时提供了对于每个节点的**监控与通知机制**。

ZooKeeper集群是一个基于**主从复制**的高可用集群，每个服务器承担如下三种角色中的一种：

- **Leader** 一个ZooKeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的**写操作必须**要通过Leader完成再由Leader将写操作广播给其它服务器。
- **Follower** 一个ZooKeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的**读请求**，同时会**将写请求转发给Leader处理**，并且负责在Leader**处理写请求时对请求进行投票**。
- **Observer** 角色与Follower类似，但是**无投票权**。

## ZAB 协议

Zab协议 的全称是 **Zookeeper Atomic Broadcast** （Zookeeper原子广播）。**Zookeeper 是通过 Zab 协议来保证分布式事务的最终一致性**。

1. Zab协议是为分布式协调服务Zookeeper专门设计的一种 **支持崩溃恢复** 的 **原子广播协议** ，是Zookeeper保证数据一致性的核心算法。Zab借鉴了Paxos算法，但又不像Paxos那样，是一种通用的分布式一致性算法。**它是特别为Zookeeper设计的支持崩溃恢复的原子广播协议**。
2. 在Zookeeper中主要依赖Zab协议来实现数据一致性，基于该协议，zk实现了一种主备模型（即Leader和Follower模型）的系统架构来保证集群中各个副本之间数据的一致性。

Zookeeper 客户端会**随机的**链接到 zookeeper 集群中的一个节点，如果是读请求，就直接从当前节点中读取数据；如果是写请求，那么节点就会向 Leader 提交事务，Leader 接收到事务提交，会广播该事务，**只要超过半数节点写入成功，该事务就会被提交**。

**Zab 协议的特性**：

1）Zab 协议需要确保那些**已经在 Leader 服务器上提交（Commit）的事务最终被所有的服务器提交**。

2）Zab 协议需要确保**丢弃那些只在 Leader 上被提出而没有被提交的事务**。

## 消息广播

在zookeeper集群中，数据副本的传递策略就是采用消息广播模式。zookeeper中农数据副本的同步方式与二段提交相似，但是却又不同。ZAB协议中Leader只要半数以上的Follower成功反馈即可。消息广播具体步骤：

1）客户端发起一个写操作请求。

2）Leader 服务器将客户端的请求**转化为事务 Proposal 提案**，同时为每个 Proposal 分配一个**全局的ID，即zxid**。

3）Leader 服务器为每个 Follower 服务器**分配一个单独的队列**，然后将需要广播的 Proposal 依次放到队列中取，并且**根据 FIFO 策略**进行消息发送。

4）Follower 接收到 Proposal 后，会首先将其**以事务日志的方式写入本地磁盘中**，写入成功后向 Leader **反馈一个 Ack 响应消息**。

5）Leader 接收到**超过半数以上** Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。

6）Leader 向所有 Follower 广播 commit 消息，同时**自身也会完成事务提交**。Follower 接收到 commit 消息后，会将上一条事务提交。

**zookeeper 采用 Zab 协议的核心，就是只要有一台服务器提交了 Proposal，就要确保所有的服务器最终都能正确提交 Proposal。这也是 CAP/BASE 实现最终一致性的一个体现。**

**Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦。 Leader 和 Follower 之间只需要往队列中发消息即可。如果使用同步的方式会引起阻塞，性能要下降很多。**

**这里要注意：**

- Leader并**不需要得到Observer的ACK**，即Observer无投票权
- Leader不需要得到所有Follower的ACK，只要**收到过半的ACK即可**，**同时Leader本身对自己有一个ACK**。
- Observer虽然无投票权，但仍须同步Leader的数据从而在处理读请求时可以返回尽可能新的数据

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608143837.png)

Follower/Observer均可接受写请求，但不能直接处理，而需要将写请求**转发给Leader处理**

## 崩溃恢复

**一旦 Leader 服务器出现崩溃或者由于网络原因导致 Leader 服务器失去了与过半 Follower 的联系，那么就会进入崩溃恢复模式。**

在 Zab 协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的 Leader 服务器。因此 Zab 协议需要一个高效且可靠的 Leader 选举算法，从而确保能够快速选举出新的 Leader 。

Leader 选举算法不仅仅需要让 Leader 自己知道自己已经被选举为 Leader ，同时还需要让集群中的所有其他机器也能够快速感知到选举产生的新 Leader 服务器。

崩溃恢复主要包括两部分：**Leader选举** 和 **数据恢复**

## 选举

到3.4.10版本为止，可选项有：

- 0 基于UDP的LeaderElection
- 1 基于UDP的FastLeaderElection
- 2 基于UDP和认证的FastLeaderElection
- 3 **基于TCP的FastLeaderElection**

在3.4.10版本中，默认值为3，也即基于TCP的FastLeaderElection。另外三种算法已经被弃用，并且有计划在之后的版本中将它们彻底删除而不再支持。

### myid

每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群**唯一的ID**（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其ID与hostname必须一一对应，如下所示。在该配置文件中，server.后面的数据即为myid

```properties
server.1=zoo1:2888:3888
server.2=zoo2:2888:3888
server.3=zoo3:2888:3888
```

### zxid

类似于RDBMS中的事务ID，用于**标识一次更新操作的Proposal ID**。为了保证顺序性，该zkid必须**单调递增**。

因此ZooKeeper使用**一个64位的数**来表示，**高32位是Leader 周期的epoch编号**，从1开始，每次选出新的Leader，epoch加一。

**低32位为该epoch内的序号**，针对客户端**每一个事务请求**，Leader 在产生新的 Proposal 事务时，都会对该计数器加1。**每次epoch变化**，都将低32位的序号**重置**。这样保证了**zxid的全局递增性**。

**Zab 协议通过 epoch 编号来区分 Leader 变化周期**，能够有效避免不同的 Leader 错误的使用了相同的 zxid 编号提出了不一样的 Proposal 的异常情况。

### 服务器状态

- **LOOKING** 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举。
- **FOLLOWING** 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁。
- **LEADING** 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳。
- **OBSERVING** 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票。

### 选票数据结构

每个服务器在进行领导选举时，会发送如下关键信息：

- **logicClock** 每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的**第多少轮投票**
- **state** 当前服务器的状态
- **self_id** 当前服务器的myid
- **self_zxid** 当前服务器上所保存的数据的最大zxid
- **vote_id** 被推举的服务器的myid
- **vote_zxid** 被推举的服务器上所保存的数据的最大zxid

### 选举步骤

**成为 Leader 的条件：**

- 1）**判断选举轮次**，若**logicClock**大于自己，则清空自己的票箱并更新为最新的logicClock。若小于则忽略。若相等则进行pk
- 2）**leader周期（epoch ）判断**，选择epoch最大的选票。
- 2）**zxid判断**，比对zxid的低32位，选择最新（最大）的zxid。
-  3）若 epoch 和 zxid 相等，选择 server_id 最大的（myid）

**Zookeeper 规定所有有效的投票都必须在同一个轮次（logicClock）中每个服务器在开始新一轮投票时，都会对自己维护的 logicalClock 进行自增操作**。

每个服务器在广播自己的选票前，会将自己的投票箱（recvset）清空。该投票箱记录了所受到的选票。

例如：Server_2 投票给 Server_3，Server_3 投票给 Server_1，则Server_1的投票箱为(2,3)、(3,1)、(1,1)。（每个服务器都会默认给自己投票）

前一个数字表示投票者，后一个数字表示被选举者。**票箱中只会记录每一个投票者的最后一次投票记录**，如果投票者更新自己的选票，则其他服务器收到该新选票后会在自己的票箱中**更新该服务器的选票**。

节点在选举开始时，都默认投票给自己，当接收其他节点的选票时，会根据上面的 **Leader条件** 判断并且更改自己的选票，然后重新发送选票给其他节点。**当有一个节点的得票超过半数，该节点会设置自己的状态为 Leading ，其他节点会设置自己的状态为 Following**。

**Recovery Phase（恢复阶段）：**

这一阶段 Follower 发送他们的 lastZxid 给 Leader，**Leader 根据 lastZxid 决定如何同步数据**。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608151851.png)

## Zab 协议如何保证数据一致性

假设两种异常情况：

1、一个事务在 Leader 上提交了，并且过半的 Folower 都响应 Ack 了，但是 Leader 在 Commit 消息发出之前挂了。

 2、假设一个事务在 Leader 提出之后，Leader 挂了。

要确保如果发生上述两种情况，数据还能保持一致性，那么 Zab 协议选举算法必须满足以下要求：

**Zab 协议崩溃恢复要求满足以下两个要求**：

1）**确保已经被 Leader 提交的 Proposal 必须最终被所有的 Follower 服务器提交**。

 2）**确保丢弃已经被 Leader 提出的但是没有被提交的 Proposal**。

根据上述要求

Zab协议需要保证选举出来的Leader需要满足以下条件：

1）**新选举出来的 Leader 不能包含未提交的 Proposal** 。
 即新选举的 Leader 必须都是已经提交了 Proposal 的 Follower 服务器节点。

 2）**新选举的 Leader 节点中含有最大的 zxid** 。
 这样做的好处是可以避免 Leader 服务器检查 Proposal 的提交和丢弃工作。

## ZAB 与 CAP 

ZooKeeper是一种分布式系统，它在一致性上有人认为它提供的是一种**强一致性**的服务（**通过sync操作**），也有人认为是**单调一致性**（更新时的大多说概念），还有人为是最终一致性（顺序一致性），反正各有各的道理这里就不在争辩了。然后它在分区容错性和可用性上做了一定折中，这和CAP理论是吻合的。ZooKeeper从以下几点保证了数据的一致性

**① 顺序一致性**

来自任意特定客户端的更新都会按其发送顺序被提交。也就是说，如果一个客户端将Znode z的值更新为a，在之后的操作中，它又将z的值更新为b，则没有客户端能够在看到z的值是b之后再看到值a（如果没有其他对z的更新）。

**② 原子性**

每个更新要么成功，要么失败。这意味着如果一个更新失败，则不会有客户端会看到这个更新的结果。

**③ 单一系统映像**

一个客户端无论连接到哪一个服务器都能看到**完全一样的系统镜像**（即完全一样的树形结构）。注：根据上文《ZooKeeper架构及FastLeaderElection机制》介绍的 ZAB 协议，**写操作并不保证更新被所有的 Follower 立即确认**，因此通过部分 Follower **读取数据并不能保证读到最新的数据**，而部分 Follwer 及 Leader 可读到最新数据。如果一定要保证单一系统镜像，**可在读操作前使用 sync 方法**。

**④ 持久性**

一个更新一旦成功，其结果就会持久存在并且不会被撤销。这表明更新不会受到服务器故障的影响。

> 一致性分类：
>
> **① 强一致性**（strong consistency）。任何时刻，任何用户都能读取到最近一次成功更新的数据。
>
> **② 单调一致性**（monotonic consistency）。任何时刻，任何用户一旦读到某个数据在某次更新后的值，那么就不会再读到比这个值更旧的值。也就是说，可获取的数据顺序必是单调递增的。
>
> **③ 会话一致性**（session consistency）。任何用户在某次会话中，一旦读到某个数据在某次更新后的值，那么在本次会话中就不会再读到比这个值更旧的值。会话一致性是在单调一致性的基础上进一步放松约束，只保证单个用户单个会话内的单调性，在不同用户或同一用户不同会话间则没有保障。
>
> **④** **最终一致性**（eventual consistency）。用户只能读到某次更新后的值，但系统保证数据将最终达到完全一致的状态，只是所需时间不能保障。
>
> **⑤ 弱一致性**（weak consistency）。用户无法在确定时间内读到最新更新的值。

## zookeeper 的watch机制

Znode发生变化（Znode本身的增加，删除，修改，以及子Znode的变化）可以通过Watch机制通知到客户端。那么要实现Watch，就必须实现org.apache.zookeeper.Watcher接口，并且将实现类的对象传入到可以Watch的方法中。

在所有读操作中，如果需要Watcher，我们可以**自定义Watcher**，如果是Boolean型变量，当为true时，则使用系统默认的Watcher，系统默认的Watcher是在Zookeeper的构造函数中定义的Watcher。参数中Watcher为空或者false，表示不启用Wather。

- **一次性触发器**

  客户端在Znode设置了Watch时，如果Znode内容发生改变，那么客户端就会获得Watch事件。但是节点再次发生变化，那客户端是无法收到Watch事件的，除非客户端设置了新的Watch。

- **发送至客户端**

  Watch事件是异步发送到Client。Zookeeper可以保证客户端发送过去的**更新顺序是有序的**。**我们使用Zookeeper不能期望能够监控到节点每次的变化。Zookeeper只能保证最终的一致性，而无法保证强一致性。**

- 设置watch的数据内容

  Znode改变有很多种方式，例如：节点创建，节点删除，节点改变，子节点改变等等。Zookeeper维护了**两个Watch列表**，**一个节点数据Watch列表**，**另一个是子节点Watch列表**。getData()和exists()设置数据Watch，getChildren()设置子节点Watch，两者选其一。

**watch的运行机制：**

- 1，Watch是**轻量级的**，其实就是本地JVM的Callback，服务器端只是存了是否有设置了Watcher的布尔类型。

- 2，在服务端，在FinalRequestProcessor处理对应的Znode操作时，会根据客户端传递的watcher变量，添加到对应的ZKDatabase （org.apache.zookeeper.server.ZKDatabase）中进行持久化存储，同时将自己NIOServerCnxn做为一个Watcher callback，**监听服务端事件**变化

- 3，Leader通过**投票通过**了某次Znode变化的请求后，然后通知对应的Follower，Follower根据自己内存中的zkDataBase信息，发送notification信息给zookeeper客户端。

- 4，Zookeeper客户端接收到notification信息后，找到对应变化path的watcher列表，**挨个进行触发回调**。

  ![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200608203352)

## 总结

ZAB 协议和 Raft 协议实际上是有相似之处的，比如都有一个 Leader，用来保证一致性（Paxos 并没有使用 Leader 机制保证一致性）。再有采取过半即成功的机制保证服务可用（实际上 Paxos 和 Raft 都是这么做的）。

ZAB 让整个 Zookeeper 集群在两个模式之间转换，**消息广播和崩溃恢复**，消息广播可以说是一个**简化版本的 2PC**，**通过崩溃恢复**解决了 2PC 的单点问题，**通过队列**解决了 2PC 的同步阻塞问题。

而支持崩溃恢复后数据准确性的就是数据同步了，数据同步基于事务的 ZXID 的唯一性来保证。通过 + 1 操作可以辨别事务的先后顺序。

# raft 与 zab

在Redis-cluster、etcd、RocketMQ的DLedger（4.5版本新加）中采用Raft算法。

在zookeeper中采用ZAB算法。

**相同点：**

1. 采用 quorum 来确定整个系统的一致性,这个 quorum 一般实现是集群中半数以上的服务器,
2. zookeeper 里还提供了带权重的 quorum 实现.
3. 都由 leader 来发起写操作.
4. 都采用心跳检测存活性
5.  leader election 都采用先到先得的投票方式

**不同点：**

1. zab 用的是 epoch 和 count 的组合来唯一表示一个值, 而 raft 用的是 term 和 index
2. zab 的 follower 在投票给一个 leader 之前必须和 leader 的日志达成一致。而 raft 的 follower则简单地说是谁的 term 高就投票给谁
3. raft 协议的心跳是从 leader 到 follower, 而 zab 协议则相反
4. 恢复方向：raft单向，仅从leader到follower补齐log；zab双向，leader需要从follower接收数据来生成initial history
5. zab的follower可以接收请求并直接响应读请求，而raft中只有leader响应所有请求
6. zab的消息广播（数据同步）采用的是两阶段方式，而raft是只有一个阶段
7. 角色的划分，zab有单独的observer角色，而raft有candidate角色

# nacos

[Nacos 注册中心的设计原理详解](https://www.infoq.cn/article/B*6vyMIKao9vAKIsJYpE?utm_source=tuicool&utm_medium=referral)

etcd：https://www.cnblogs.com/softidea/p/6517959.html

# 总结

从上面的讨论上来看，**半数**很关键（半数提交、半数选举），相信看到这里你会懂得这半数的魔力，接下来总结一下此分布式一致性协议由于”半数”会有哪些问题：

- 事务操作的**性能瓶颈**：由于需要保证半数节点保存了数据，则增删改数据的性能取决于半数节点的性能（5个节点的写入性能则取决于3个节点（包括自己）的写入速度）和与半数节点通信的网络开销。这意味着，集群节点越多（如果想要高可用，那就上多节点），写入性能有可能会越差（试想，上100个节点，每次写入都要发给50个节点确保写入成功，有50次网络开销，不过数据复制过程是并行的，**木桶效应**总耗时取决于半数中最慢的那个节点，不过节点越多，木桶短板出现的概率也就越大，所以这里说写入性能是可能变差的）

- **基本可用**：集群由于需要半数提交才能成功写入数据，所以如果5个节点宕机2个集群是可用的，若宕机3个，由于得不到半数的提交，集群就会不可用，从这点上来看达到了基本可用的特性

- **最终一致性** or **强一致性**（线性一致性）：

  如果我们仅仅只是写入一个值，从写入角度来说那就是强一致性（因为都在Leader处理，Raft与Zab都支持顺序一致性，即为你刚刚Set M = 1，又Set M = 2，顺序如果反了那结果会变成 M = 1，由于都在同一个Leader节点做，节点维护一个队列即可保持顺序），所以下面更多的是对于读数据的一致性讨论

  - 最终一致性：这取决于具体的实现，若是最终一致性，那么**Follower就开放读能力**，这么做有个优点，整个集群的读能力会随着节点个数得到提升，但有个缺点，由于半数提交的规则，有可能上一秒你写入成功了一个值，下一秒在集群另外某个节点中读不到这个值（可能还没同步到），但最终会在一个时间点此值被同步，也就是最终一定会达到一致性
  - 强一致性（**线性一致性**）：如果读能力只在Leader节点开放，也就是读我也只能在Leader上读，那么此时的一致性是极强的，随着上一秒的设置值，下一秒一定可以查到刚刚设置的值，缺点就是读性能得不到扩展，局限于Leader单点性能的问题（读写都在一台节点上操作）

  以上讨论了CP模型，对一致性要求比较高的系统比较适用，可以看出来，这种分布式协议有性能的局限性，因为他牺牲了一定的客户端响应延迟来确保一致性，而且仅仅保证了半数的基本可用性。在延时要求高、高并发场景下这种模型或许并不适用，此时可以考虑AP模型提高响应延迟。

如果是AP模型，相比于一致性协议会简单一些，因为他只需要保证可用性，加集群节点即可，而数据丢失、不一致的情况只会在宕机的那一时刻发生，丢失、不一致的也只是那一时刻的数据，例如**Redis的主从集群**架构，主Redis节点只需要**异步**、定时去同步数据，在写入时**只需要一个节点确认写入**即可返回，延时比一致性协议低，由于Redis在使用上大部分场景都用在缓存，快是他的设计目标，偶尔丢几条或者不一致几条缓存数据并不影响场景。

- 为什么Nacos配置中心要使用CP模型？

答：CP模型牺牲了一定的延时去换取数据的强一致，但应用的外部化配置并不要求配置成功的延迟要多低，更多的是要保证配置信息的一致性（我配置什么信息A=1，不要给我弄丢了或者等下查 A 不等于 1，这些都是数据不一致，这肯定是不行的），所以在这种配置场景下是十分适合做CP模型的

- 为什么Nacos注册中心要使用AP模型？

答：这个问题也可以转换成为什么注册中心要使用AP模型。因为可用性比一致性更加重要，可用性体现在两个方面，第一是容许集群宕机数量，AP模型中集群全部机器宕机才会造成不可用，CP模型只容许一半的机器宕机。第二是分区是否可写，例如A机房和B机房发生分区，无法进行网络调用，在CP模型下部署在A机房的服务就无法部署，因为A机房无法与B机房通讯，所以无法写入IP地址，在AP模型下，可以去中心化，就像Eureka那样就算发生分区每台机器还都能写入，集群间无Master，而是互相复制各自的服务IP信息，这样，在分区时依然是支持写入的。不可用会造成微服务间调用拿不到IP地址，进而业务崩盘，所以不可用是不可接受的，不一致会造成不同微服务拿到的IP地址不一样，会造成负载不均衡，这是可以接受的。

原文链接：https://blog.csdn.net/weixin_36425412/article/details/113315782