# 基础知识

## 基本架构

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200526201810)

- **连接器**

  连接器主要负责客户端与数据库服务的连接工作。可通过命令**show processlist** 查询当前可用的连接进程的状态。客户端连接后，如果长时间没有通讯连接，则会**自动断开连接**，默认wait_timeout参数的值是**8小时**。

- **查询缓存**

  执行查询语句的时候，会先查询缓存（MySQL8.0 版本后移除，因为这个功能不太实用）

  缓存的失效很容易，只要对表有任何的更新，这个表的所有查询缓存就会全部被清空，就会出现缓存还没使用，就直接被清空了，或者积累了很多缓存准备用来着，但是一个更新打回原形。

- **分析器**

  分析器主要是通过词法分析你的sql语句，用来告诉MySQL你要干什么。这个时候，如果你的sql语句有语法错误，就会报异常：“You have an error in your SQL syntax”

- **优化器**

  优化器主要是通过你的sql语句，选择一种最优的方式，告诉MySQL该如何执行该语句。

- **执行器**

  执行器通过操作引擎，将sql语句执行的结果，返回给客户端。

## 三大范式

- 第一范式

  要求数据库表的每一列都是**不可分割**的基本数据项，同一列中不能有多个值。

  若某一列有多个值，可以将该列单独拆分成一个实体，新实体和原实体间是一对多的关系。

- 第二范式

  在第一范式的基础上，非主属性必须完全依赖于主键。

  主键可能由多个属性构成，完全依赖要求不允许存在非主属性依赖于主键中的某一部分属性。若存在哪个非主属性依赖于主键中的一部分属性，那么要将发生部分依赖的这一组属性单独新建一个实体。

- 第三范式

  在第二范式的基础上，实体中的属性不能是其他实体中的非主属性。因为这样会出现冗余。即：属性不依赖于其他非主属性。

>  在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会**为了性能而妥协数据库的设计**。

## 相关的权限表

MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容：

- user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。
- db权限表：记录各个帐号在各个数据库上的操作权限。
- table_priv权限表：记录数据表级的操作权限。
- columns_priv权限表：记录数据列级的操作权限。
- host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。

## binlog

有三种格式，**statement**，**row**和**mixed**。

- statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。

- row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来。但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。

  > 新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。

- **mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。**

Binlog日志格式选择：**Mysql默认是使用Statement日志格式，推荐使用MIXED。**

由于一些特殊使用，可以考虑使用ROWED，**如自己通过binlog日志来同步数据的修改**，这样会节省很多相关操作。对于binlog数据处理会变得非常轻松,相对mixed，解析也会很轻松(当然前提是增加的日志量所带来的IO开销在容忍的范围内即可)。 

在slave日志同步过程中，对于使用**now()**这样的时间函数，MIXED日志格式，会在日志中产生对应的unix_timestamp()*1000的时间字符串，slave在完成同步时，取用的是sqlEvent发生的时间来保证数据的准确性。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200526205005)

## 数据类型

### 整型

| 类型名称       | 说明                                 |
| -------------- | ------------------------------------ |
| tinyInt        | 1个字节 范围(-128~127)               |
| smallint       | 2个字节 范围(-32768~32767)           |
| mediumint      | 3个字节 范围(-8388608~8388607)       |
| int(integer)   | 4个字节 范围(-2147483648~2147483647) |
| bigint（long） | 8个字节 范围(+-9.22*10的18次方)      |

取值范围如果加了**unsigned**，则最大值翻倍，如tinyint unsigned的取值范围为(0~256)。

 int(m)里的m是表示SELECT查询结果集中的**显示宽度**，并**不影响实际的取值范围**。可选显示宽度规定用于显示宽度小于指定的列宽度的值时从左侧填满宽度。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200526222913.png)



### 浮点型（float和double）

| 类型名称     | 说明                                                         |
| ------------ | ------------------------------------------------------------ |
| float(m,d)   | 单精度浮点型  8位精度(4字节)   m总个数，d小数位              |
| double(m,d)  | 双精度浮点型  16位精度(8字节)   m总个数，d小数位             |
| decimal(m,d) | 定点数，存放的是精确值，而浮点数是近似值。m<65 是总个数，d<30且 d<m 是小数位 |

设一个字段定义为float(6,3)，如果插入一个数123.45678,实际数据库里存的是123.457，但总个数还以实际为准，即6位。整数部分最大是3位，如果插入数12.123456，存储的是12.1234，如果插入12.12，存储的是12.1200.

### 字符串

| 类型名称   | 说明                            |
| ---------- | ------------------------------- |
| char(n)    | 固定长度，最多255个字符         |
| varchar(n) | 可变长度，最多65535个字符       |
| tinytext   | 可变长度，最多255个字符         |
| text       | 可变长度，最多65535个字符       |
| mediumtext | 可变长度，最多2的24次方-1个字符 |
| longtext   | 可变长度，最多2的32次方-1个字符 |

- **char和varchar的区别**

  - char(n) 固定长度，char(4)不管是存入几个字符，都将占用4个字节。

    varchar是存入的实际字符数+1个字节（n<=255）或2个字节(n>255)，所以varchar(4),存入3个字符将占用4个字节。 

  - char类型的字符串检索速度要比varchar类型的快。

- varchar和text的区别
  - varchar可指定n，text不能指定，内部存储varchar是存入的实际字符数+1个字节（n<=255）或2个字节(n>255)，text是实际字符数+2个字节。
  - text类型不能有默认值。
  - varchar可直接创建索引，text创建索引要指定前多少个字符。varchar查询速度快于text,在都创建索引的情况下，text的索引似乎不起作用。 

### 二进制（blob）

| 类型名称   | 说明                     |
| ---------- | ------------------------ |
| blob       | 允许长度0~65535字节      |
| mediumblob | 允许长度0~167772150字节  |
| longblob   | 允许长度0~4294967295字节 |

- blob和text存储方式不同，text以文本方式存储，英文存储区分大小写，而blob是以二进制方式存储，不分大小写。

- blob存储的数据只能整体读出。 

- text可以指定字符集，blob不用指定字符集。

### 日期类型

| 类型名称  | 说明                                                         |
| --------- | ------------------------------------------------------------ |
| date      | 日期 '2008-12-2'                                             |
| time      | 时间 '12:25:36'                                              |
| datetime  | 8个字节，日期时间 '2008-12-2 22:06:44'                       |
| timestamp | 4个字节，自动存储记录修改时间，日期时间 '2008-12-2 22:06:44' |

- **datetime与timestamp的区别**
  - datetime占用8个字节，timestamp占用4个字节。**timestamp利用率更高**。
  - datetime的默认值为null，timestamp的默认值不为null，且为系统当前时间（current_timestatmp）。如果存进去的是NULL，timestamp会**自动储存当前时间**。
  - **二者存储方式不一样**，对于timestamp，它把客户端插入的时间从当前时区转化为世界标准时间（UTC）进行存储，查询时，逆向返回。但对于datetime，基本上存什么是什么。
  - **二者范围不一样**。timestamp范围：‘1970-01-01 00:00:01.000000’ 到 ‘2038-01-19 03:14:07.999999’； datetime范围：’1000-01-01 00:00:00.000000’ 到 ‘9999-12-31 23:59:59.999999’。原因是，timestamp占用4字节，能表示最大的时间毫秒为2的31次方减1，也就是2147483647，换成时间刚好是**2038**-01-19 03:14:07.999999。

### 如何选择合适数据类型

- **整形比字符操作代价更低。**
- 保存**日期格式**时，要选用MySQL内置的日志格式（TimeStamp, DateTime）,而不使用字符串。PS:TimeStamp使用4个字节进行存储（存储的时间从1970到2034），而DateTime使用8个字节存储（时间从1001到9999）,上述两种时间格式**推荐使用TimeStamp**. 上述的两种日期格式的**精度都只是到秒**，如果要存储到毫秒或者更高的精度，可以考虑用BIGINT来保存。
- **小数点的保存**，如果涉及到运算，用float和double可能会精度不准，可以考虑将小数乘于相应的倍数，然后用**BIGINT**来存储。
- **整数**，如果没有负数的情况，则加入**UNSIGNED**可以使保存的范围数（在正数范围内）提升一倍。
- **CHAR和VARCHAR**，如果是**定长字符**推荐CHAR（N），CHAR的优势是经常更新不会产生太多的碎片。

- **关于保存ip地址**。对于通常的ipv4，比如：192.168.1.123，我们会用15位char来存储这种是不推荐的，实际上ipv4是32位的无符号整数，推荐**将ipv4转成整数**进行保存。在MySQL中提供了INET_ATON()将ipv4转成整形以及INET_NTOA()将整形转成ipv4

- **默认值的缺省值的设置**，**尽量不用NULL**，可以设置为“”或者0。

# 存储引擎

## 存储引擎分类

常用的存储引擎有三种：

- Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是**处理大数据容量的数据库系统**。
- MyISAM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。
- MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。

> - MyISAM管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的SELECT查询，那么MyISAM是更好的选择。
> - InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能。
>
> 上述答案算是在那个年代的一种总结。如今MYSQL已经发展到5.7的版本了，这些回答已经完全失去了意义，有时想这个百度搜索什么时候能按时间排序，或者百度搜索能不能对一些明显错误或者过时的内容进行摒弃啊，现在在百度里搜索MYISAM与InnoDB区别时，出来的都是几年前的答案，这明显在误导人啊！
>
> 从MySQL 5.5开始，**InnoDB存储引擎已经完胜MyISAM了**，已经看不到任何其他应用**使用MyISAM的必要性**。当然，**MyISAM存储引擎本身已经彻底停止开发了**。从MYSQL5.5开始InnoDB将作为默认存储引擎，MySQL专家摩根·托克(Morgan Tocker)表示，这实际上是一个重大改变。它将引发MySQL使用方式的巨变。过去用户通常是最初使用MyISAM存储引擎，然后学习转向数据管理功能更强大的InnoDB;现在是从一开始就使用更高级和更复杂的存储引擎。我们将看到更多的人开始学习了解InnoDB，而知道MyISAM引擎的人则会减少很多。人们讨论的话题不再是“为什么我要从MyISAM转向InnoDB?”而是“我听说还有一个MyISAM引擎，什么情况下我应该试用它?”

## MyISAM与Innodb的区别

在MySQL 5.5之前，MyISAM是mysql的默认数据库引擎，其由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然MyISAM性能极佳，但却有一个显著的缺点： **不支持事务处理**。不过，MySQL也导入了另一种数据库引擎InnoDB，以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。

- **存储结构**：每个MyISAM在磁盘上存储成三个文件：第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义，数据文件的扩展名为.MYD (MYData)，索引文件的扩展名是.MYI (MYIndex)。

  InnoDB**所有的表都保存在同一个数据文件中**（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。

- **存储空间**：MyISAM可被压缩，占据的存储空间较小，支持静态表、动态表、压缩表三种不同的存储格式。

  InnoDB需要**更多的内存和存储**，**它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引**。

- **可移植性、备份及恢复**：MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便，同时在备份和恢复时也可单独针对某个表进行操作。

  InnoDB免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。

- **事务支持**：MyISAM强调的是性能，每次查询**具有原子性**，其执行数度比InnoDB类型更快，但是不提供事务支持。

  InnoDB提供事务、外键等高级数据库功能，具有**事务提交、回滚和崩溃修复**能力。

- **AUTO_INCREMENT**：在MyISAM中，可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，它可以根据前面几列进行排序后递增。

  InnoDB中必须包含只有该字段的索引，并且引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。

- **表锁差异**：MyISAM只支持表级锁，用户在操作MyISAM表时，select、update、delete和insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。

  InnoDB支持**事务和行级锁**。行锁大幅度提高了多用户并发操作的新能，但是InnoDB的行锁，**只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。**

- **全文索引**：MyISAM支持 FULLTEXT类型的全文索引；InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。

- **表主键**：MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。

- **表的具体行数**：MyISAM保存**表的总行数**，select count() from table;会直接取出出该值；

  而InnoDB没有保存表的总行数，如果使用select count() from table；就会**遍历整个表**，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。

- **CURD操作**：在MyISAM中，如果执行大量的SELECT，MyISAM是更好的选择。

  对于InnoDB，如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。

- **外键**：MyISAM不支持外键，而InnoDB支持外键。

通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储过程、视图、行级锁、外键等等。尤其在并发很多的情况下，InnoDB的表现肯定要比MyISAM强很多。另外，必须需要注意的是，任何一种表都不是万能的，合适的才是最好的，才能最大的发挥MySQL的性能优势。如果是不复杂的、非关键的Web应用，还是可以继续考虑MyISAM的，这个具体情况具体考虑。

## MyISAM和Innodb的索引区别

MyIASM引擎，B+树的数据结构中存储的内容实际上是实际数据的地址值。也就是说它的索引和实际数据是分开的，**只不过使用索引指向了实际数据。这种索引的模式被称为非聚集索引。**

Innodb引擎的索引的数据结构也是B+树，**只不过数据结构中存储的都是实际的数据，这种索引有被称为聚集索引。**

InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到**覆盖索引会非常高效**。

## 文件存储结构

在 MySQL中建立任何一张数据表，在其数据目录对应的数据库目录下都有对应表的 `.frm` 文件，`.frm` 文件是用来保存每个数据表的元数据(meta)信息，包括表结构的定义等，与数据库存储引擎无关，也就是任何存储引擎的数据表都必须有`.frm`文件，命名方式为 数据表名.frm，如user.frm。

查看MySQL 数据保存在哪里：`show variables like 'data%'`

MyISAM 物理文件结构为：

- `.frm`文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等
- `.MYD` (`MYData`) 文件：MyISAM 存储引擎专用，用于存储MyISAM 表的数据
- `.MYI` (`MYIndex`)文件：MyISAM 存储引擎专用，用于存储MyISAM 表的索引相关信息

InnoDB 物理文件结构为：

- `.frm` 文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等

- `.ibd` 文件或 `.ibdata` 文件：这两种文件都是存放 InnoDB 数据的文件，之所以有两种文件形式存放 InnoDB 的数据，是因为 InnoDB 的数据存储方式能够通过配置来决定是使用**共享表空间**存放存储数据，还是用**独享表空间**存放存储数据。

  独享表空间存储方式使用`.ibd`文件，并且每个表一个`.ibd`文件 共享表空间存储方式使用`.ibdata`文件，所有表共同使用一个`.ibdata`文件（或多个，可自己配置）

## Innodb 面试题

### 为什么使用B+树

#### 二叉树

二叉查找树也称为**有序二叉查找树**，在树平衡的情况下，时间复杂度是O(logn)，但是二叉树的树是不平衡的，在极端情况下会变成一个单向链表（有序的插入），这是的时间复杂度就会变成O(n)

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200527000132)



#### 平衡二叉树（AVL树或红黑树）

平衡二叉树可以满足查询、插入、删除等操作的时间复杂度是O(logn)，但是树的高度会比较高，当查询节点时需要多次IO操作，频繁的随机读，对于IO性能影响很大。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200527000618)

#### hash表

Hash表的查找效率是O(1)（在hash冲突不严重的情况下，如果极端情况则会变成O(n)），但是数据并不是有序的，在范围查找时就需要遍历所有，复杂度变成O(n)

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200527001025)

#### B树

B树是多路查找树，在查找效率上是O(logn)，并且相比于二叉树又可以很好的控制树高，但相比于B+树还是有一定的不足。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200527002020)

#### B+树

优点（相比于B树）：

- 由于B+树在内部节点上**不包含数据信息**，因此在内存页中能够**存放更多的key**。数据存放的更加紧密，具有更好的空间局部性。因此访问叶子节点上关联的数据也具有更好的**缓存命中率**。
- B+树的**叶子结点都是相连的**，因此对整棵树的遍历只需要一次线性遍历叶子结点即可。而且由于数据**顺序排列并且相连**，所以**便于区间查找和搜索**。  而B树则需要进行每一层的递归遍历，相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200527002234.jpg)

### 可以存放多少数据

在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，**一个块的大小是4k**，而对于我们的InnoDB存储引擎也有自己的最小储存单元——页（**Page**），**一个页的大小是16K**。

假设一行数据的大小是1k，那么一个页可以存放**16行**这样的数据。

假设主键ID为**bigint**类型，**长度为8字节，而指针大小在InnoDB源码中设置为6字节**，这样一共14字节，我们一个页中能存放多少这样的单元，其实就代表有多少指针，即16384/14=1170。那么可以算出一棵高度为2的B+树，能存放**1170*16=18720**条这样的数据记录。

根据同样的原理我们可以算出一个高度为3的B+树可以存放：**1170\*1170\*16=21902400条**这样的记录。所以在InnoDB中B+树高度一般为1-3层，它就能满足千万级的数据存储。在查找数据时**一次页的查找代表一次IO**，所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。

> 为什么是16K？因为这和pagecache有关，当读取数据时，会把整页（4K）数据加载到内存中，同时默认会读入随后的几个page（通常是三个），所以每个节点设置为16K，可以确保每次可以把节点的数据都加载到内存中。

### Innodb页结构

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200526235436)

- **各个数据页**可以组成一个**双向链表**
- 而**每个数据页中的记录**又可以组成一个**单向**链表
- 每个数据页都会为存储在它里边儿的记录生成一个**页目录**，在通过**主键**查找某条记录的时候可以在页目录中使用**二分法快速定位**到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录
- 以**其他列**(非主键)作为搜索条件：只能从最小记录开始**依次遍历单链表中的每条记录**。

### count

对于无条件的count会造成**全表扫描**（基于索引），在myisam中会记录表的总行数，但是在innodb中因为事务隔离的原因（事务的可见性），并**未记录总行数**。

Mysql的查询优化器针对无条件的count(*)或count(1)做了优化，会**选择成本最小的辅助索引**进行扫描。针对有条件的count就需要根据where条件确定执行计划。

> 这里的成本主要指：
>
> - IO成本：即从磁盘把数据加载到内存的成本，默认情况下，**读取数据页的 IO 成本是 1**
> - CPU成本：将数据读入内存后，还要检测数据是否满足条件和排序等 CPU 操作的成本，显然它与行数有关，默认情况下，**检测记录的成本是 0.2**。

- **count(*)、count(1)与count(column)的区别**

  count的定义：大致的定义是返回 SELECT 语句检索的行中 expr 的非 NULL 值的计数，到这里我们就明白了，首先它是一个**聚合函数**，然后对 SELECT 的结果集进行计数，但是需要**参数不为 NULL**。

  count(\*)不同，他**不关心这个返回值是否为空都会计算他的count**，因为 count(1) 中的 1 是**恒真表达式**，那么 count(*) 还是 count(1) 都是对所有的结果集进行 count，所以他们**本质上没有什么区别**。

  count(column) 也是会遍历整张表，但是不同的是它会拿到 column 的值以后**判断是否为空**，然后再进行累加，那么如果针对主键需要解析内容，如果是二级索引需要再次根据主键获取内容，又是一次 IO 操作，所以 count(column) 的性能肯定不如前两者，如果按照效率比较的话：

  **count(*) = count(1) > count(primary key) > count(column)**

- **判断数据是否存在的正确做法**

  目前大部分人在判断数据是否存在时使用的是count查询，再根据数量判断数据是否存在。但是该用法存在较大的性能风险，因为count是需要（全表）扫描的。正确的做法：

  ```sql
  # 返回常量，并且限制limit 1，可以确保当扫描到第一条数据时就会停下来
  SELECT 1 FROM table WHERE a = 1 AND b = 2 LIMIT 1
  ```

### 崩溃恢复能力

在sql（事务）提交时，不仅在缓存中（buffer pool）记录，是一个单向链表，基于LRU淘汰机制。同时会在磁盘上记录**redo log**，由于是顺序写，性能是很高的。在出现故障时可以基于redo log进行数据恢复。在合适的机会会批量刷数据到磁盘，目的是为了减少随机IO带来的性能损耗

同时有**double write**机制，如果在写磁盘的过程中崩溃，那么page数据会不完整，而redo log记录的是**物理变化**，无法基于redo log进行恢复。所以在刷数据时，会先将数据写入到**doubleWrite buffer**，然后再开始写磁盘。在恢复时会先利用 Doublewrite Buffer 来修复磁盘里的数据，然后再基于redo log进行数据恢复。

redo log和doubleWrite机制都可以保障数据的可靠性。

### 行锁，加锁机制

**行级锁并不是直接锁记录，而是锁索引，如果一条SQL语句用到了主键索引，mysql会锁住主键索引；如果一条语句操作了非主键索引，mysql会先锁住非主键索引，再锁定主键索引。**这也是死锁的主要原因。

**假设where条件并未走索引，而是全表扫描，将导致全表锁。**

### 联合索引的底层存储结构

单列索引其实也可以看做联合索引，索引列为1的联合索引，从下图就可以看出联合索引的底层存储跟单列索引类似的，区别在于联合索引是**每个树节点中包含多个索引值**，在通过索引查找记录时，会先将联合索引中第一个索引列与节点中第一个索引值进行匹配，匹配成功接着匹配第二个索引列和索引值，直到联合索引的所有索引列都匹配完；如果过程中出现某一个索引列与节点相应位置的索引值不匹配的情况，则无需再匹配节点中剩余索引列，前往下一个节点。

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200612160359)

# 事务

## 四大特性（ACID）

![事物的特性](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200529002433)



1. **原子性（Atomicity）：** 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
2. **一致性（Consistency）：** 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；
3. **隔离性（Isolation）：** 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
4. **持久性（Durability）：** 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。

## 并发事务带来的问题

在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
- **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。
- **不可重复读（Unrepeatableread）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

**不可重复度和幻读区别：**

不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。

## 事务隔离级别

**SQL 标准定义了四个隔离级别：**

- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。

- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。

- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。

- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

------

|     隔离级别     | 脏读 | 不可重复读 | 幻影读 |
| :--------------: | :--: | :--------: | :----: |
| READ-UNCOMMITTED |  √   |     √      |   √    |
|  READ-COMMITTED  |  ×   |     √      |   √    |
| REPEATABLE-READ  |  ×   |     ×      |   √    |
|   SERIALIZABLE   |  ×   |     ×      |   ×    |

MySQL InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）**。我们可以通过`SELECT @@tx_isolation;`命令来查看

```
mysql> SELECT @@tx_isolation;
+-----------------+
| @@tx_isolation  |
+-----------------+
| REPEATABLE-READ |
+-----------------+
```

这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 **REPEATABLE-READ（可重读）**事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）** 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的**SERIALIZABLE(可串行化)**隔离级别。

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是**READ-COMMITTED(读取提交内容):**，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）**并不会有任何性能损失。

## 如何保证四大特性

原子性、一致性、持久性通过数据库的redo log和undo log来完成。**redo log称为重做日志，用来保证事务的原子性和持久性。undo log用来保证事务的原子性和一致性。**

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200603160702.png)

# MVCC 多版本并发控制

MVCC（Multi-Version Concurrency Control）即多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，**实现对数据库的并发访问**。MVCC使得大部分支持行锁的事务引擎，**不再单纯的使用行锁来进行数据库的并发控制，取而代之的是把数据库的行锁与行的多个版本结合起来**，只需要很小的开销，就可以实现**非锁定读**，从而**大大提高数据库系统的并发性能**。

如果有人从数据库中读数据的同时，有另外的人写入数据，有可能读数据的人会看到『半写』或者不一致的数据。有很多种方法来解决这个问题，叫做并发控制方法。最简单的方法，通过加锁，让所有的读者等待写者工作完成，但是这样效率会很差。MVCC 使用了一种不同的手段，每个连接到数据库的读者，在某个瞬间看到的是**数据库的一个快照**，写者写操作造成的变化在写操作完成之前（或者数据库事务提交之前）对于其他的读者来说是不可见的。

基于提升并发性能的考虑，各大数据库厂商的事务型存储引擎一般都同时实现了多版本并发控制（MVCC）。不仅是MySQL，包括Oracle、PostgreSQL等其他数据库系统也都实现了。MVCC就像是Java语言中的接口，**各个数据库厂商的实现机制不尽相同**。可以认为MVCC是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了**非阻塞的读操作**，**写操作也只是锁定必要的行**。MVCC会**保存某个时间点上的数据快照**。这意味着事务可以看到一个一致的数据视图，不管他们需要跑多久。这同时也意味着不同的事务在同一个时间点看到的同一个表的数据可能是不同的。前面说到不同的存储引擎的MVCC实现是不同的，典型的有乐观并发控制和悲观并发控制。

MVCC实现的读写不阻塞正如其名：多版本并发控制---->通过一定机制生成一个数据请求时间点的**一致性数据快照**（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本。

## innodb的 MVCC 实现

在每一行数据中额外保存两个隐藏的列：当前**行创建时的版本号和删除时的版本号**（可能为空，其实**还有一列称为回滚指针**，用于事务回滚）。这里的版本号并不是实际的时间值，而是系统版本号。每开始新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询每行记录的版本号进行比较。每个事务又有自己的版本号，这样事务内执行CRUD操作时，就**通过版本号的比较来达到数据版本控制的目的**。

innoDBb存储的最基本row中包含一些额外的存储信息 DATA_TRX_ID、DATA_ROLL_PTR、DB_ROW_ID、DELETE BIT。

- DATA_TRX_ID标记了最新更新这条行记录的transaction id，每处理一个事务，其值自动+1
- DATA_ROLL_PTR 指向当前记录项的rollback segment的undo log记录，找之前版本的数据就是通过这个指针
- DB_ROW_ID，当由innodb自动产生聚集索引时，聚集索引包括这个DB_ROW_ID的值，否则聚集索引中不包括这个值，这个用于索引当中
- DELETE BIT位用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除，真正意义的删除是在commit的时候。

1、初始插入数据行

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200603173246.png)

F1～F6是某行列的名字，1～6是其对应的数据。后面三个隐含字段分别对应该行的事务号和回滚指针，假如这条数据是刚INSERT的，可以认为ID为1，其他两个字段为空

2、事务1更改该行的各字段的值

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200603173338.png)

当事务1更改该行的值时，会进行如下操作：

- 用排他锁锁定该行
- 记录redo log
- 把该行修改前的值Copy到undo log，即上图中下面的行
- 修改当前行的值，填写事务编号，使回滚指针指向undo log中的修改前的行

3、事务2修改该行的值

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200603173413.png)

与事务1相同，此时undo log中有两行记录，并且**通过回滚指针连在一起**。因此，如果undo log一直不删除，则会通过当前记录的回滚指针**回溯到该行创建时的初始内容**，所幸的是在Innodb中存在purge线程，它会查询那些比现在最老的活动事务还早的undo log，并删除它们，从而保证undo log文件不至于无限增长。

当事务正常提交时只需要更改事务状态为COMMIT即可，不需做其他额外的工作，而Rollback则稍微复杂点，需要**根据当前回滚指针从undo log中找出事务修改前的版本并恢复**。如果事务影响的行非常多，回滚则可能会变的效率不高，根据经验值没事务行数在1000～10000之间，Innodb效率还是非常高的。很显然，Innodb是**一个COMMIT效率比Rollback高的存储引擎**。

## 快照读和当前读

快照读就是读取数据的时候会根据一定规则**读取事务可见版本的数据**（可能是过期的数据），**不用加锁**。

当前读, **读取的是最新版本, 并且对读取的记录加锁**，保证其他事务不会再并发的修改这条记录，避免出现安全问题。

使用当前读的场景：

- select…lock in share mode (共享读锁)
- select…for update
- update
- delete
- insert

使用快照读的场景：

- 单纯的select操作，不包括上述 select … lock in share mode、select … for update

当你执行select *之后，在A与B事务中都会返回4条一样的数据，这是不用想的，**RR隔离级别下当执行普通的select查询时，innodb默认会执行快照读**，相当于就是给你目前的状态找了一张照片，以后执行select 的时候就会返回当前照片里面的数据，当其他事务提交了也对你不造成影响，和你没关系，这就**实现了可重复读**，那这个快照是什么时候生成的呢？**不是开启事务的时候，是当你第一次执行select的时候**，也就是说，当A开启了事务，然后没有执行任何操作，这时候B insert了一条数据然后commit，这时候A在事务中执行select，那么就能看到有B在自己在事务中添加的那条数据…，在这之后无论再有其他事务commit都没有关系，因为照片已经生成了，而且不会再生成了，以后都会参考这个快照。

## 总结

众所周知地是更新（update、insert、delete）是一个事务过程，在Innodb中，查询也是一个事务，只读事务。**当读写事务并发访问同一行数据时**，能读到什么样的内容则依赖事务级	别：

- READ_UNCOMMITTED，读未提交，**读事务直接读取主记录**，无论更新事务是否完成
- READ_COMMITTED，读已提交，**读事务每次都读取距离undo log最近的那个版本**，因此两次对同一字段的读可能读到不同的数据（幻读），但能保证每次都读到最新的数据
- REPEATABLE_READ，**每次都读取指定的版本**，这样保证不会产生幻读，但可能读不到最新的数据
- SERIALIZABLE，锁表，**读写相互阻塞**，使用较少

Innodb的实现方式是：

- 事务以**排他锁的形式修改原始数据**
- 把修改前的数据存放于undo log，通过**回滚指针与主数据关联**
- 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback）

MVCC可以保证**不阻塞地读到一致的数据**。但是MVCC理论并没有对实现细节做约束，为此不同的数据库的语义有所不同，比如：

- postgres **对写操作也是乐观并发控制**；在表中保存同一行数据记录的多个不同版本，每次写操作都是创建，而回避更新；在事务提交时，按版本号检查当前事务提交的数据**是否存在写冲突，则抛异常告知用户**，回滚事务；
- innodb 则**只对读无锁，写操作仍是上锁的悲观并发控制**，这也意味着，innodb中只能见到**因死锁和不变性约束而回滚**，而见不到因为写冲突而回滚；不像 postgres 那样对数据修改在表中创建新纪录，而是每行数据只在表中保留一份，在更新数据时上行锁，同时将旧版数据写入 undo log；表和 undo log 中行数据都记录着事务ID，在检索时根据事务隔离级别去读取行数据。可见 MVCC中的写操作仍可以按悲观并发控制实现；

# 锁

## 锁分类

**MyISAM和InnoDB存储引擎使用的锁：**

- MyISAM采用表级锁(table-level locking)。
- InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁

**表级锁和行级锁对比：**

- **表级锁：** MySQL中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。
- **行级锁：** MySQL中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。

从锁的类别又分为：

- **共享锁**: 又叫做读锁。当用户要进行数据的读取时，对数据加上共享锁。**共享锁可以同时加上多个**。
- **排他锁**: 又叫做写锁。当用户要进行数据的写入时，对数据加上排他锁。**排他锁只可以加一个，他和其他的排他锁，共享锁都相斥**。

## 锁算法

**InnoDB存储引擎的锁的算法有三种：**

- **Record lock**：

  单条索引记录上加锁，record lock**锁住的永远是索引**，而非记录本身，即使该表上没有任何索引，那么innodb会在后台创建一个隐藏的聚集主键索引，那么锁住的就是这个隐藏的聚集主键索引。所以说当一条sql**没有走任何索引时，那么将会在每一条聚集索引后面加X锁，这个类似于表锁**。

- Gap lock：

  在索引记录之间的间隙中加锁，或者是在某一条索引记录**之前或者之后**加锁，**并不包括该索引记录本身**。

- Next-key lock：

  **record+gap 锁定一个范围，包含记录本身**

**相关知识点：**

1. **innodb对于行的查询使用next-key lock**
2. Next-locking keying**为了解决Phantom Problem幻读问题**
3. 当查询的**索引含有唯一属性时，将next-key lock降级为record key**
4. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
5. 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

### next-key lock

**在默认情况下，mysql的事务隔离级别是可重复读，并且innodb_locks_unsafe_for_binlog参数为0，这时默认采用next-key locks。所谓Next-Key Locks，就是Record lock和gap lock的结合，即除了锁住记录本身，还要再锁住索引之间的间隙。**下面主要介绍间隙锁：

```powershell
Create Table: CREATE TABLE `test_gap_lock` (
  `id` int(11) NOT NULL,
  `name` varchar(100) DEFAULT NULL,
  `myid` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `uniq_name` (`name`),
  KEY `idex_myid` (`myid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8
1 row in set (0.00 sec)

# 表数据
mysql> select * from test_gap_lock;
+-----+------------+------+
| id  | name       | myid |
+-----+------------+------+
|   1 | jiang      |   98 |
|   5 | hubingmei4 |  101 |
|   6 | jiang2     |  100 |
|  67 | jiang222   |   80 |
+-----+------------+------+
9 rows in set (0.00 sec)

# 第一步操作，开启事务，读取myid=100的数据，这时会加上行级共享锁
mysql> begin;
mysql> select * from test_gap_lock where myid=100

# session 2 插入myid=99的记录依旧阻塞，存在gap锁；
mysql> insert into test_gap_lock values(676,'gap recored test',99);
ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction

# 插入myid=97的记录成功
mysql> insert into test_gap_lock values(675,'gap recored test1',97);
Query OK, 1 row affected (0.00 sec)
```

**过程分析：**

- 基于myid为维度，数据分为（-∞, 80）、（80，98）、（98，100）、（100， 101）几个区间段
- 当对myid=100的数据加锁时，由于next-key lock机制，那么同时会对（98，100）、（100， 101）**两个区间的数据加区间锁（gap lock）**
- 由于myid=99是在（98，100）区间内，所以导致需要锁等待

为什么要加gap锁：

**由于myid是非主键索引、非唯一索引，所以才会加gap区间锁，是为了避免幻读。**

假如现在插入一条myid=100的数据（并不是唯一索引）,那么如果没有区间锁的话，就会导致幻读（读取到两条myid=100）的数据。

> **gap锁只会阻塞insert操作**，因为gap间隙中是不存在任何记录的，除了insert操作，其他的操作结果应该都等价于空操作，mysql就不去阻塞它了

## 常见操作是如何加锁的

在默认情况下，mysql的事务隔离级别是**可重复读**，并且innodb_locks_unsafe_for_binlog参数为0，这时**默认采用next-key locks**。所谓Next-Key Locks，就是Record lock和gap lock的结合，即除了锁住记录本身，还要再锁住索引之间的间隙。

下面我们针对大部分的SQL类型分析是如何加锁的，假设事务隔离级别为**可重复读**。

- **select .. from**  

  不加任何类型的锁

- **select...from lock in share mode**

  在扫描到的**任何索引记录上加共享的（shared）next-key lock，还有主键聚集索引加共享锁 ** 

- **select..from for update**

  在扫描到的**任何索引记录上加排它的next-key lock，还有主键聚集索引加排它锁** 

- **update..where  delete from..where**

  在扫描到的**任何索引记录上加next-key lock，还有主键聚集索引加排它锁** 

- **insert into..**

  简单的insert会在insert的行对应的索引记录上加一个排它锁，这是一个**record lock，并没有gap**，所以并不会阻塞其他session在gap间隙里插入记录。

  不过在insert操作之前，还会加一种锁，官方文档称它为insertion intention gap lock，也就是**意向的gap锁**。这个意向gap锁的作用就是预示着当多事务并发插入相同的gap空隙时，只要插入的记录**不是gap间隙中的相同位置**，则无需等待其他session就可完成，这样就使得insert操作无须加真正的gap lock。想象一下，如果一个表有一个索引idx_test，表中有记录1和8，那么每个事务都可以在2和7之间插入任何记录，只会对当前插入的记录加record lock，并不会阻塞其他session插入与自己不同的记录，因为他们并没有任何冲突。

## 意向锁

innodb的意向锁主要**用户多粒度的锁并存**的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，**逐行检查锁标志的开销将很大**，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。

举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。

说白了意向锁的主要作用是**处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁”**

## 乐观锁与悲观锁

**悲观锁：**假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在**查询完数据的时候就把事务锁起来，直到提交事务**。实现方式：使用数据库中的锁机制

**乐观锁：**假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。update table_x  num=num+1 where num=xx

两种锁的**使用场景**：

从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于**写比较少**的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。

但如果是**多写的情况**，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。

## 死锁

**死锁产生**：

- 死锁是指两个或多个事务**在同一资源上相互占用**，并请求锁定对方占用的资源，从而导致恶性循环
- 当事务试图**以不同的顺序锁定资源时**，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁
- 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。

**检测死锁**：数据库系统实现了各种死锁检测和死锁超时的机制。**InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误**。

**死锁恢复**：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，InnoDB目前处理死锁的方法是，**将持有最少行级排他锁的事务进行回滚**。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。

**外部锁的死锁检测**：发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决

**死锁影响性能**：**死锁会影响性能而不是会产生严重错误**，因为InnoDB会自动检测死锁状况并回滚其中一个受影响的事务。在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。有时当发生死锁时，禁用死锁检测（使用innodb_deadlock_detect配置选项）可能会更有效，这时可以依赖`innodb_lock_wait_timeout`设置进行事务回滚。

**MyISAM避免死锁**：

- 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。

**InnoDB避免死锁**：

- 为了在单个InnoDB表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个数据（行）使用`SELECT ... FOR UPDATE`语句来获取必要的锁，即使这些行的更改语句是在之后才执行的。--提前用排它锁锁定资源
- 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，**而不应先申请共享锁、更新时再申请排他锁**，因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，**从而造成锁冲突，甚至死锁**
- 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。在应用中，如果不同的程序会并发存取多个表，**应尽量约定以相同的顺序来访问表**，这样可以大大降低产生死锁的机会
- 通过`SELECT ... LOCK IN SHARE MODE`获取行的读锁后，如果当前事务再需要对该记录进行更新操作，**则很有可能造成死锁**。
- 改变事务隔离级别

如果出现死锁，可以用 `show engine innodb status;`命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。

# 索引

## 简述

索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里**所有记录的引用指针**。

索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常**使用B+树或HASH表**。

索引的**优点**

- **大大加快数据的检索速度**，这也是创建索引的最主要的原因；
- 加速表和表之间的连接；
- 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间；
- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性；

索引的**缺点**

- **时间方面**：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；
- **空间方面**：索引需要占物理空间。

**索引的分类**

- **唯一索引**：唯一索引不允许两行具有相同的索引值
- **主键索引**：为表定义一个主键将自动创建主键索引，主键索引是唯一索引的特殊类型。主键索引要求主键中的每个值是唯一的，并且不能为空
- **聚集索引(Clustered)**：表中各行的物理顺序与键值的逻辑（索引）顺序相同，每个表只能有一个
- **非聚集索引(Non-clustered)**：非聚集索引指定表的逻辑顺序。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于249个

## 创建索引的原则

索引虽好，但也不是无限制的使用，最好符合一下几个原则

1） **最左前缀匹配原则**，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2）**较频繁作为查询条件**的字段才去创建索引

3）**更新频繁字段不适合创建索引**

4）若是**不能有效区分数据的列**不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)

5）**尽量的扩展索引，不要新建索引**。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。

8）对于定义为text、image和bit的数据类型的列不要建立索引。

## 创建索引需要注意什么？

- **非空字段**：**Mysql难以优化引用可空列查询，它会使索引、索引统计和值更加复杂。可空列需要更多的存储空间**，还需要mysql内部进行特殊处理。可空列被索引后，**每条记录都需要一个额外的字节**，还能导致MyISAM中固定大小的索引变成可变大小的索引。应该**用0、一个特殊的值或者一个空串代替空值**；
- **取值离散大的字段**：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；
- **索引字段越小越好**：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。

**为什么不建议使用NULL：**

（1）所有使用NULL值都可以通过一个有意义的值表示，这样也有利代码的可读性和可维护性，并能从约束上增强业务数据的规范性。

（2）NULL值到非NULL值的更新无法做到原地更新，更容易发生索引分裂，从而影响性能。

（3）not in、!=查询在有NULL值的情况下返回永远为空结果，查询容易出错。

（4）Null列需要更多的存储空间：需要一个额外字节作为判断是否为Null的标志位

## 索引一定可以提高查询性能吗？

通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。

索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。**因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢**。使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况:

- **基于一个范围的检索**，一般查询返回结果集**小于表中记录数的30%**
- **基于非唯一性索引的检索**

## 最左匹配原则

顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中**使用最频繁的一列放在最左边**。

最左前缀匹配原则，**非常重要的原则**，mysql会一直向右匹配**直到遇到范围查询(>、<、between、like)就停止匹配**，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

**=和in可以乱序**，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的**查询优化器**会帮你优化成索引可以识别的形式

## 覆盖索引

区分聚簇索引和非聚簇索引：

- 聚簇索引：**将数据存储与索引放到了一块**，找到索引也就找到了数据
- 非聚簇索引：**将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行**。非聚簇索引都是**辅助索引**，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的**不再是行的物理位置，而是主键值**。

在基于非聚簇索引查询时，只能获取到主键值，但是其他信息需要到聚簇索引中查询，这叫做**回表查询**。覆盖索引就是为了避免回表查询，select中的字段已经在非聚簇索引中了，不需要再回表。类似于select age from user where age>20；

## 索引下推

对于user_table表，我们现在有（username,age）联合索引。如果现在有一个需求，查出名称中以“张”开头且年龄小于等于10的用户信息，语句C如下："select * from user_table where username like '张%' and age > 10"。语句C有两种执行可能：
 根据（username,age）联合索引查询所有满足名称以“张”开头的索引，然后回表查询出相应的全行数据，然后再筛选出满足年龄小于等于10的用户数据。

2、根据（username,age）联合索引查询所有满足名称以“张”开头的索引，然后直接再筛选出年龄小于等于10的索引，之后再回表查询全行数据。

明显的，第二种方式**需要回表查询的次数比较少**，这就是**mysql的索引下推**。mysql默认启用索引下推，我们也可以通过修改系统变量optimizer_switch的index_condition_pushdown标志来控制

```mysql
SET optimizer_switch = 'index_condition_pushdown=off';
```

注意点：
 1、innodb引擎的表，索引下推只能用于**二级索引**，并且是**联合索引**。

2、索引下推一般可用于所求查询字段（select列）**不是/不全是联合索引的字段**，查询条件为多条件查询且查询条件子句（where/order by）字段全是联合索引。

> 假设表t有联合索引（a,b）,下面语句可以使用索引下推提高效率
> select * from t where a > 2 and b > 10;

# explain

[一本彻底搞懂MySQL索引优化EXPLAIN百科全书](https://mp.weixin.qq.com/s/QCJq1o-CWbNNwnuzJmVEPg)

| **`列名`**    | **`用途`**                                             |
| ------------- | ------------------------------------------------------ |
| id            | 每一个SELECT关键字查询语句都对应一个唯一id             |
| select_type   | SELECT关键字对应的查询类型                             |
| table         | 表名                                                   |
| partitions    | 匹配的分区信息                                         |
| type          | 单表的访问方法                                         |
| possible_keys | 可能用到的索引                                         |
| key           | 实际使用到的索引                                       |
| key_len       | 实际使用到的索引长度                                   |
| ref           | 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 |
| rows          | 预估需要读取的记录条数                                 |
| filtered      | 某个表经过条件过滤后剩余的记录条数百分比               |
| Extra         | 额外的一些信息                                         |

## id 列

select 查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序

- id相同，执行顺序从上往下
- id全不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行
- id部分相同，执行顺序是先按照数字大的先执行，然后数字相同的按照从上往下的顺序执行

## select_type 列

每一个 `SELECT` 关键字的查询都定义了一个 `select_type` 属性，知道这个查询属性就能知道在整个查询语句中所扮演的角色。

- **SIMPLE** ：简单的select查询，查询中不包含子查询或UNION
- **PRIMARY**：查询中若包含任何复杂的子部分，最外层查询被标记为PRIMARY
- **SUBQUERY**：在select或where列表中包含了子查询
- **DERIVED**：在from列表中包含的子查询被标记为DERIVED，MySQL会递归执行这些子查询，把结果放在临时表里
- **UNION**：若第二个select出现在UNION之后，则被标记为UNION，若UNION包含在from子句的子查询中，外层select将被标记为DERIVED
- **UNION RESULT**：从UNION表获取结果的select

## table 列

`table` 列表示 `EXPLAIN` 的单独行的唯一标识符。这个值**可能是表名、表的别名或者一个未查询产生临时表的标识符，如派生表、子查询或集合**。

当 `FROM` 子句中有子查询时，如果优化器采用的物化方式，table 列是 `<derivenN>` 格式，表示当前查询依赖 `id=N` 的查询，于是先执行 `id=N` 的查询。

当使用 `UNION` 查询时，`UNION RESULT` 的 table 列的值为 `<UNION1,2>`，1和2表示参与 `UNION` 的 SELECT 的行 id。

## type 列-重点

这一列表示**关联类型或访问类型**，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 一般来说，**得保证查询达到range级别**，最好达到ref 。依次从最优到最差分别为：

> **system  >  const  >  eq_ref  >  ref  >  range  >  index  >  ALL**

### 1）system,const

MySQL 能对查询的某部分进行优化并将其**转化成一个常量**。用于**主键或唯一二级索引列与常数比较时**，所以表最多有一个匹配行，`读取1次，速度比较快`。`system`是 `const` 的特例，表里只有一条记录匹配时为 `system`。

```sql
EXPLAIN SELECT *FROM (SELECT* FROM user where id = 1) tmp;
```

### 2）eq_ref

在**连接查询**时，如果**被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的**，则对该被驱动表的访问方法就是 `eq_ref`。这可能是在 const 之外最好的联接类型了。

```sql
EXPLAIN SELECT * FROM user_group INNER JOIN user ON user_group.user_id = user.id;
```

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200530160523)

### 3）ref

相比 eq_ref，不使用唯一索引，而是**使用普通索引或者唯一性索引的部分前缀**，索引要和某个值相比较，可能会找到多个符合条件的行。

### 4）ref_or_null

对普通二级索引进行等值查询，该索引列也可以为NULL值时。

```sql
EXPLAIN SELECT * FROM user where user.name = 'a' OR name IS NULL;
```

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200530160537)

### 5）index_merge

MySQL使用**索引合并**的方式执行的。

```sql
EXPLAIN SELECT * FROM user WHERE user.name = 'a' OR user.id = 1;
```

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200530160807)

### 6）range

**使用索引获取`范围区间`的记录，通常出现在 `in, between ,> ,<, >=` 等操作中**。

```sql
EXPLAIN SELECT * FROM user WHERE user.id > 1;
```

![img](https://520li.oss-cn-hangzhou.aliyuncs.com/img/20200530160910)

### 7）index

**扫描全表索引**，这通常比ALL快一些。（`index`是从索引中读取的，而 `ALL` 是从硬盘中读取）

### 8）ALL

**全表扫描**，MySQL 需要从头到尾去查找表中所需要的行。通常情况下这需要增加索引来进行优化了。

## possible_keys 列

`possible_keys` 列表示查询**可能使用哪些索引来查找**。

`EXPLAIN` 执行计划结果可能出现 `possible_keys` 列，而 `key` 显示 `NULL` 的情况，这种情况是因为表中数据不多，MySQL 会认为索引对此查询帮助不大，选择了全表查询。

如果 `possible_keys` 列为 `NULL`，则没有相关的索引。在这种情况下，可以通过检查 `WHERE` 子句去分析下，看看是否可以创造一个适当的索引来提高查询性能，然后用 `EXPLAIN` 查看效果。

另外**注意**：不**是这一列的值越多越好，使用索引过多，查询优化器计算时查询成本高，所以如果可能的话，尽量删除那些不用的索引。**

## key 列

`key` 列表示实际**采用哪个索引**来优化对该表的访问。

如果没有使用索引，则该列是 NULL。如果想强制 MySQL使用或忽视 `possible_keys` 列中的索引，在查询中使用 `force index`、`ignore index`。

## key_len 列

`key_len` 列表示当查询优化器决定使用某一个索引查询时，**该索引记录的最大长度**。

## ref 列

`ref` 列显示了在 `key` 列记录的索引中，表查找值所用到的列或常量，常见的有：`const`（常量），`字段名`（例：`user.id`）。

## rows 列

`rows` 列是查询优化器**估计要读取并检测的行数**，注意这个不是结果集里的行数。

如果查询优化器使用全表扫描查询，`rows` 列代表预计的需要扫码的行数；如果查询优化器使用索引执行查询，`rows` 列代表预计扫描的索引记录行数。

## filtered 列

对于单表来说意义不大，**主要用于连接查询中**。

前文中也已提到 `filtered` 列，**是一个百分比的值**，对于连接查询来说，主要看`驱动表`的 `filtered`列的值 ，通过 **`rows * filtered/100` 计算可以估算出`被驱动表`还需要执行的查询次数**。

## Extra 列

`Extra` 列提供了**一些额外信息**。这一列在 MySQL中提供的信息有几十个，这里仅列举一些常见的重要值如下：

1. **using filesort**:

   说明mysql会对数据**使用一个外部的索引排序**，不是按照表内的索引顺序进行读取。mysql中无法利用索引完成的排序操作称为“文件排序”。常见于order by和group by语句中

2. **Using temporary**：

   **使用了临时表保存中间结果**，mysql在对查询结果排序时使用临时表。常见于排序order by和分组查询group by。

3. **using index**：

   表示相应的select操作中**使用了覆盖索引，**避免访问了表的数据行，效率不错，如果同时出现using where，表明索引被用来执行索引键值的查找；否则索引被用来读取数据而非执行查找操作

4. using where：使用了where过滤

5. using join buffer：使用了连接缓存

6. impossible where：where子句的值总是false，不能用来获取任何元祖

7. select tables optimized away：在没有group by子句的情况下，基于索引优化操作或对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化

8. distinct：优化distinct操作，在找到第一匹配的元祖后即停止找同样值的动作

9. Using index condition：使用了**索引下推**

# 查询优化器

## 物化

子查询语句中的子查询结果集中的记录**保存到临时表**的过程称之为 `物化`（英文名：`Materialize`），简单理解为存储子查询结果集的临时表称之为 `物化表`。

也正因为物化表的记录都建立了索引（**基于内存的物化表有哈希索引，基于磁盘的有B+树索引**），因此通过 `IN` 语句判断某个操作数在不在子查询的结果集中变得很快，从而提升语句的性能。

## 半连接 semi-join

也是跟 `IN` 语句子查询有关。

通用语句：

```
SELECT ... FROM outer_tables
    WHERE expr IN (SELECT ... FROM inner_tables ...) AND ...
```

`outer_tables` 表对 `inner_tables` 半连接的意思：

对于`outer_tables`的某条记录来说，我们仅关心在`inner_tables`表中是否存在匹配的记录，而不用关心具体有多少条记录与之匹配，最终结果只保留 outer_tables 表的记录。

# 优化 [参考](https://blog.csdn.net/qq_16059847/article/details/90231424)

## 1）慢查询日志

MySQL 的慢查询日志是 MySQL 提供的一种日志记录，它用来记录在 MySQL 中响应时间超过阈值的语句，具体指运行时间超过 `long_query_time` 值的 SQL，则会被记录到慢查询日志中。

- `long_query_time` 的默认值为10，意思是运行10秒以上的语句
- 默认情况下，MySQL数据库没有开启慢查询日志，需要手动设置参数开启

**查看开启状态**

```
SHOW VARIABLES LIKE '%slow_query_log%'
```

**开启慢查询日志**

- 临时配置：

```
mysql> set global slow_query_log='ON';
mysql> set global slow_query_log_file='/var/lib/mysql/hostname-slow.log';
mysql> set global long_query_time=2;
```

也可set文件位置，系统会默认给一个缺省文件host_name-slow.log

使用set操作开启慢查询日志只对当前数据库生效，如果MySQL重启则会失效。

- 永久配置

  修改配置文件my.cnf或my.ini，在[mysqld]一行下面加入两个配置参数

```
[mysqld]
slow_query_log = ON
slow_query_log_file = /var/lib/mysql/hostname-slow.log
long_query_time = 3
```

注：log-slow-queries 参数为慢查询日志存放的位置，一般这个目录要有 MySQL 的运行帐号的可写权限，一般都将这个目录设置为 MySQL 的数据存放目录；long_query_time=2 中的 2 表示查询超过两秒才记录；在my.cnf或者 my.ini 中添加 log-queries-not-using-indexes 参数，表示记录下没有使用索引的查询。

可以用 `select sleep(4)` 验证是否成功开启。

在生产环境中，如果手工分析日志，查找、分析SQL，还是比较费劲的，所以MySQL提供了日志分析工具**mysqldumpslow**。

通过 mysqldumpslow --help 查看操作帮助信息

- 得到返回记录集最多的10个SQL

  `mysqldumpslow -s r -t 10 /var/lib/mysql/hostname-slow.log`

- 得到访问次数最多的10个SQL

  `mysqldumpslow -s c -t 10 /var/lib/mysql/hostname-slow.log`

- 得到按照时间排序的前10条里面含有左连接的查询语句

  `mysqldumpslow -s t -t 10 -g "left join" /var/lib/mysql/hostname-slow.log`

- 也可以和管道配合使用

  `mysqldumpslow -s r -t 10 /var/lib/mysql/hostname-slow.log | more`

**如何优化：**

1. 首先分析语句，看看是否**load了额外的数据**，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。
2. **分析语句的执行计划**，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。
3. 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行**横向或者纵向的分表**。

## 2）SQL优化

- 1）索引。避免一张表的索引数量超过5个，如果业务允许的情况下**尽量使用组合索引**。单次SQL只会使用一个索引
- 2）join。在设计表时尽量从业务角度出发，表字段有一定的冗余，**避免大表之间的关联查询**。结果集一致的情况下尽量使用join而不是left join；
- 3）在left join时，**可以提前缩小范围**，通过子查询的方式再关联，
- 4）**利用小表驱动大表**，如from A left join B，则尽量保障A表的行数或满足条件的记录少，这样可以减少关联算法次数。注：目前的mysql优化器支持**自行优化关联顺序**
- 5）**避免使用子查询，可以用join替代**，因为子查询的结果集是放在临时表中，无法使用索引，产生大量临时表而影响性能。如常见的exist、in
- 6）避免使用select * 。减少IO开销，且减少*转换字段的开销
- 7）**使用in替代or**。但是in的值最好不要超过1000个，如果比较长的话可以使用union all连接，或临时表；
- 8）尽量使用union all而不是union，因为union会对结果集进行去重操作；
- 9）数据量大时，**limit的性能**是逐渐下降的，可以使用id比较替代limit分页；
- 10）避免字段隐式类型转换，如number，但传入字符串；where从句中避免对列进行函数转换和计算，以避免索引失效
- 11）对于联合索引，遵循最左原则；联合索引时，如果存在范围查询（<、>等），会造成后面索引字段失效；
- 12）char与varchar区别，char是定长的，适用于存储一些固定长度的数据（如电话），查询性能会比较好。varchar是不定长的，可能会导致存储碎片
- 13）datetime与timestamp，timestamp与时区关联，存储的是转化后的数据，返回时又再次根据时区转化成数据

## 3）索引失效的场景

- 1）使用like时的%问题；
- 2）联合索引的最左原则；
- 3）使用or（用in代替）；
- 4）字段类型导致索引失效，如字符串传入的为数字；
- 5）where从句中对列使用函数计算或转换
- 6）is null ,is not null 无法使用索引
- 7）<>，not in ，!= 会导致全表扫描

## 4）limit 优化

- 1）**使用上次的最后一条数据标识作为条件**，如：select * from users where id>100073 order by id asc limit 20，但只适用于排序字段是唯一的

- 2）利用B+Tree的特性，使用子查询，先查出对应的数据ID，再使用in或join过滤，这样不需要回表查询，且把搜索转换成了主键ID的搜索，如：

  ```sql
  # 原始
  select * From table_name Where user = xxx limit 10000, 10;
  
  select * From table_name Where id in (Select id From table_name where user = xxx ) limit 10000, 10;
  
  select * from table_name inner join ( select id from table_name where (user = xxx) limit 10000,10) b using (id)
  ```

  利用覆盖索引的特性，减少回表查询，减少load的数据。

- 3）反向查找法，当偏移量超过一半记录数时，就从后面反向查询，但是比较麻烦

- 4）limit限制，当偏移量大于某个数时就不再查询，返回空数据

## 5）in 和 exists 区别

mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。

```sql
# in查询分析
SELECT   *  FROM A WHERE id IN (SELECT id FROM B);

# 等价于
SELECT id FROM B ----->先执行in中的查询

SELECT *  FROM A  WHERE A.id = B.id
```

```sql
# exists查询分析
SELECT * FROM a WHERE EXISTS(SELECT 1 FROM b WHERE B.id  = A.id);

# 等价于
SELECT * FROM A;

SELECT I FROM B WHERE B.id = A.id;
```

**总结：**

1. 如果查询的两个表大小相当，那么用in和exists差别不大。

2. 如果两个表中一个较小，一个是大表，则**子查询表大的用exists，子查询表小的用in**。

   > 例如：
   >
   > 1、A表有100条记录,B表有1000条记录,那么EXISTS()会执行100次去判断A表中的id是否与B表中的id相等.因为它只执行A.length次，可见B表数据越多,越适合EXISTS()发挥效果.
   >
   > 2、A表有10000条记录,B表有100条记录,那么EXISTS()还是执行10000次,此时不如使用in()遍历10000*100次,**因为IN()是在内存里遍历数据进行比较**,而EXISTS()需要查询数据库,我们都知道查询数据库所消耗的性能更高,而内存比较很快.
   >
   > 3、结论：exists()适合B表比A表数据大的情况，当A表数据与B表数据一样大时,in与exists效率差不多,可任选一个使用

3. not in 和not exists：如果查询语句**使用了not in，那么内外表都进行全表扫描，没有用到索引**；而**not extsts的子查询依然能用到表上的索引**。所以无论那个表大，**用not exists都比not in要快**。

## 6）百万级别或以上的数据如何删除

关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。

1. 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）
2. 然后删除其中无用数据（此过程需要不到两分钟）
3. 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。
4. 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

## 7）连接池

[数据库连接池到底应该设多大？这篇文章可能会颠覆你的认知](https://mp.weixin.qq.com/s/dQFSrXEmgBMh1PW835rlwQ)

### 计算公式

下面的公式是由PostgreSQL提供的，不过我们认为可以广泛地应用于大多数数据库产品。你应该模拟预期的访问量，并从这一公式开始测试你的应用，寻找最合适的连接数值。

> 连接数 = ((核心数 * 2) + 有效磁盘数)

核心数不应包含超线程(hyper thread)，即使打开了hyperthreading也是。如果活跃数据全部被缓存了，那么有效磁盘数是0，随着缓存命中率的下降，有效磁盘数逐渐趋近于实际的磁盘数。这一公式作用于SSD时的效果如何尚未有分析。

按这个公式，你的4核i7数据库服务器的连接池大小应该为((4 * 2) + 1) = 9。取个整就算是是10吧。是不是觉得太小了？跑个性能测试试一下，我们保证它能轻松搞定3000用户以6000TPS的速率并发执行简单查询的场景。如果连接池大小超过10，你会看到响应时长开始增加，TPS开始下降。扩展：[用了这么久的数据库连接池，你知道原理吗？](http://mp.weixin.qq.com/s?__biz=MzI4Njc5NjM1NQ==&mid=2247490668&idx=2&sn=229c7bf8df9a3750eeb68b4eeee38a8f&chksm=ebd62340dca1aa56678800efeeae3b54649bdbed8472cd1340f39b02bb20ddc262b8ae61cc69&scene=21#wechat_redirect)

> 笔者注：
> 这一公式其实不仅适用于数据库连接池的计算，大部分涉及计算和I/O的程序，线程数的设置都可以参考这一公式。我之前在对一个使用Netty编写的消息收发服务进行压力测试时，最终测出的最佳线程数就刚好是CPU核心数的一倍。

### 公理：你需要一个小连接池，和一个充满了等待连接的线程的队列

如果你有10000个并发用户，设置一个10000的连接池基本等于失了智。1000仍然很恐怖。即是100也太多了。你需要一个10来个连接的小连接池，然后让剩下的业务线程都在队列里等待。连接池中的连接数量应该等于你的数据库能够有效同时进行的查询任务数（通常不会高于2*CPU核心数）。

我们经常见到一些小规模的web应用，应付着大约十来个的并发用户，却使用着一个100连接数的连接池。这会对你的数据库造成极其不必要的负担。

### 请注意

连接池的大小最终与系统特性相关。

比如一个混合了**长事务**和**短事务**的系统，通常是任何连接池都难以进行调优的。最好的办法是创建两个连接池，一个服务于长事务，一个服务于短事务。

再例如一个系统执行一个任务队列，只允许一定数量的任务同时执行，此时并发任务数应该去适应连接池连接数，而不是反过来。